# Ch 9_비지도 학습

- 컴퓨터 과학자 얀 르쿤이 말하길,

  > "지능이 케이크라면 비지도 학습은 케이크의 빵이고, 지도 학습은 케이크 위의 크림이고, 강화 학습은 케이크 위의 체리다."



- 대부분의 머신러닝 어플리케이션은 지도 학습 기반이지만, 실제 데이터는 대부분 레이블이 없음.
  - 레이블이 없는 데이터에 레이블을 붙이기 위해서라도 비지도 학습은 필요함.



- 비지도 학습의 예

  - **군집 Clustering** : 비슷한 샘플을 클러스터로 모음.
    - 데이터 분석, 고객 분류, 추천 시스템, 검색 엔진, 이미지 분할, 준지도 학습, 차원 축소 등에 사용.
  - **이상치 탐지 Outlier Detection** : '정상' 데이터가 어떻게 보이는지를 학습, 비정상 샘플을 감지하는 데 사용함.
    - 예를 들면, 제조 라인에서 결함 제품을 감지하거나 시계열 데이터에서 새로운 트렌드를 찾음.

  - **밀도 추정 Density Estimation** : *데이터셋 생성 확률 과정 Random process* 의 ***확률 밀도 함수 Probability Density Function, PDF*** 를 추정함.
    - 밀도 추청은 이상치 탐지에 널리 사용 됨.
      - 밀도가 매우 낮은 영역의 샘플이 이상치일 가능성이 높음.
    - 또한 데이터 분석과 시각화에도 유용.



- k-평균과 DBSCAN을 사용한 군집에서 시작, 가우시안 혼합 모델을 설명하고, 이를 밀도 추청, 군집, 이상치 탐지에 사용하는 방법을 살펴볼 것.







## 9.1 군집

- 비슷한 샘플들을 구별해, 하나의 클러스터로 할당하는 작업.
  - 분류와 비슷하지만, 분류와 달리 군집은 비지도 학습.



- *고객 분류*
  - 구매 이력이나 웹사이트 내 행동 등을 기반으로 클로스터로 모음.
    - 예를 들어 동일한 클러스터 내의 사용자가 좋아하는 컨텐츠를 추천하는, 추천 시스템



- *데이터 분석*
  - 새로운 데이터셋을 분석할 때 클러스터를 만듦



- *차원 축소 기법*
  - 데이터 셋에 군집 알고리즘을 적용, 각 클러스터에 대한 샘플의 **친화성 Affinity**를 측정할 수 있음.
    - 친화성은 샘플이 클러스터에 얼마나 잘 맞는지를 측정
  - 원본 특성 벡터보다 훨씬 저차원의 벡터로 분석에 사용



- *이상치 탐지*
  - 모든 클러스터에 친화성이 낮은 샘플은 이상치일 가능성이 높음.
    - 특히, 제조 분야에서 결함 감지, 또는 부정 거래 감지에 활용.

- *준지도 학습*
  - 레이블된 샘플이 적다면, 군집을 수행하고 레이블을 붙여, 이후의 지도 학습에 필요한 레이블을 제공할 수 있음.

- *검색 엔진*
  - 제시된 이미지와 비슷한 이미지를 찾아주는 검색 엔진을 만들 때 사용.
    - 사용자가 찾으려는 이미지를 제공하면, 훈련된 군집 모델을 사용해 이미지의 클러스터를 찾고, 클러스터의 모든 이미지를 반환.

- *이미지 분할*
  - 색을 기반으로 픽셀을 클러스터로 모은 다음, 각 픽셀의 색을 해당 클러스터의 평균 색으로 바꿈.
    - 이렇게 하면 물체의 윤관을 감지하기 쉬워져, 물체 탐지 및 추적 시스템에서 이미지 분할을 많이 활용함.



- 클러스터에 대한 보편적인 정의는 없고, 상황에 따라 다름.
  - 어떤 알고리즘은 **센트로이드 Centroid** 라 부르는 특정 포인트를 중심으로 모인 샘플을 찾고,
  - 어떤 알고리즘은 샘플이 밀집되어 연속된 영역을 찾고,
  - 어떤 알고리즘은 계층적으로 클러스터의 클러스터를 찾는 등 종류가 아주 많음.







### 9.1.1 k-평균

- k-평균은 반복 몇 번으로 데이터셋을 빠르고 효율적으로 클러스터로 묶을 수 있는 간단한 알고리즘.
- 로이드-포지 알고리즘이라고도 부름.



- 알고리즘이 찾을 클러스터 개수 k를 지정, 각 샘플은 클러스터에 할당, 군집에서 각 샘플에 붙일 레이블 label은 알고리즘이 샘플에 할당한 클러스터의 인덱스.

- k-평균 알고리즘은 클러스터의 크기가 많이 다르면 잘 작동하지 않음. 샘플을 클러스트에 할당할 때 센트오리드까지 거리를 고려하는 것이 전부이기 때문.



- **하드 군집 Hard clustering** : 샘플을 하나의 클러스터에만 할당
- **소프트 군집 Soft clustering** : 클러스터마다 샘플에 점수를 부여
  - 점수를 부여할 때, 샘플과 센트로이드 사이의 거리나 가우시안 방사 기저 함수와 같은 유사도 점수를 사용 할 수 있음.



- 고차원 데이터셋을, 샘플이 센트로이드에서 떨어져 있는 거리로 위치를 나타내는 방식으로 변환하면 k-차원 데이터셋을 만들 수 있는데, 이렇게 변환하면 매우 효율적인 비선형 차원 축소 기법이 됨.



- k-평균 알고리즘의 작동 원리는?
  - 센트로이드가 주어진다면,
    - 데이터셋의 모든 샘플에 가장 가까운 센트로이드의 클러스터를 할당하면 됨
  - 반대로 모든 샘플의 레이블이 주어진다면,
    - 각 클러스터에 속한 샘플의 평균을 계산하여 센트로이드를 구하면 됨.
  - 그럼 레이블이나 센트로이드가 주어지지 않으면?
    - 처음 센트로이드를 랜덤하게 선정하고(무작위로 k개의 샘플을 뽑아 그 위치를 센트로이드로 지정), 그 다음 샘플에 레이블을 할당하고 센트로이드를 업데이트하고, 이를 반복해서 센트로이드에 변화가 없을 때까지 반복.
- 이 알고리즘의 계산 복잡도는 일반적으로 샘플 개수 m, 클러스터 개수 k, 차원 개수 n에 선형적.
  - 하지만 데이터가 군집할 수 있는 구조일 때 그렇고, 최악의 경우 계산 복잡도는 샘플 개수가 지수적으로 증가할 수 있음.
    - 하지만 실전에서 이런 일은 드물고, 일반적으로 k-평균은 가장 빠른 군집 알고리즘 중 하나.



- 수렴은 보장되지만, 지역 최적점으로 수렴할 수 있음.
  - 이는 센트로이드 초기화에 달려있음. 센트로이드를 초기화를 개선해서 이런 위험을 줄여야 함.



- 센트로이드 초기화 방법
  - 다른 군집 알고리즘을 먼저 실행해서 센트로이드 위치의 근사값을 알거나
  - 랜덤 초기화를 다르게 하여 여러 번 알고리즘을 실행하고 가장 좋은 솔루션을 선택함
    - 이 경우 성능 지표를 사용. 각 샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리, 모델의 **이너셔 Inertia**를 사용함.

- k-평균 알고리즘을 향상시킨 k-평균++ 알고리즘에서는,
  - 다른 센트로이드와 거리가 먼 센트로이드를 선택하는 초기화 단계를 소개함.
  - 그리고 사이킷런의 KMeans 클래서는 이 초기화 방법을 사용.



- k-평균 속도 개선과 미니배치 k-평균
  - 삼각 부등식을 사용해서 불필요한 거리 계산을 많이 피했고,
  - 전체 데이터셋을 사용해 반복하지 않고, 각 반복마다 미니배치를 사용해 센트로이드를 조금씩 이동하는 방법으로 속도향상을 함.



- 최적의 클러스터 개수 찾기
  - 가장 작은 이너셔를 갖는 모델을 선택해도 결과가 좋지 않음. 이너셔는 k에 반비례하는데 k가 크다고 성능이 무조건 좋지 않음.
  - 최선의 클러스터 개수를 선택하기 위해, 하지만 계산 비용이 많이 드는, 방법은 **실루엣 점수 Silhouette score**
    - 실루엣 점수는 모든 샘플에 대한 **실루엣 계수 Silhouette coefficient**의 평균임.
      - 실루엣 계수는 (b-a) / max(a, b) 로 계산. a는 동일한 클러스터에 있는 다른 샘플까지의 평균 거리, b는 가장 가까운 클러스터까지 평균 거리를 의미.
    - 실루엣 계수는 -1에서 +1 사이의 값이며, +1에 가까우면 자신의 클러스터 안에 잘 속해 있고, 다른 클러스터와는 멀리 떨어져 있다는 의미, 실루엣 계수가 0에 가까우면 클러스터 경계에 위치, -1에 가까우면 이 샘플이 잘못된 클러스터에 할당되었다는 의미.
  - 모든 샘플의 실루엣 계수를 할당된 클러스터와 계숫값으로 정렬하여 그린 그래프를 **실루엣 다이어그램 Silhouette diagram**이라고 하며, 이 그래프의 높이는 클러스터가 포함하고 있는 샘플의 개수, 너비는 이 클러스터에 포함된 샘플의 정렬된 실루엣 계수를 의미(넓을수록 좋음).
    - 전반적인 클러스터의 크기가 비슷하고, 수직 파선보다 높은 계수를 가지는 경우를 선택해야 함.







### 9.1.2 k-평균의 한계

- k-평균은 특히 속도가 빠르고 확장이 용이한 장점이 있음.
  - 하지만, 최적이 아닌 솔루션을 피하려면 알고리즘을 여러 번 실행해야 하고, 클러스터 개수를 잘 지정해야하는 한계가 있으며, 클러스터의 크기나 밀집도가 서로 다르거나 원형이 아닌경우 잘 작동하지 않음.
    - 참고로, 타원평 클러스터에서는 가우시안 혼합 모델이 잘 작동함.
    - 이런 경우는 입력 특성의 스케일을 맞춰서 보정할 수 있음.







### 9.1.3 군집을 사용한 이미지 분할

- **이미지 분할 Image segmentation** : 이미지를 ***세그먼트 Segment*** 여러 개로 분할하는 작업
  - **시맨틱 분할 Semantic segmentation** : 동일한 종류의 물체에 속한 모든 픽셀을 같은 세그먼트에 할당.
  - **색상 분할 Color segmentation** : 동일한 색상을 가진 픽셀을 같은 세그먼트에 할당.







### 9.1.4 군집을 사용한 전처리

- 군집은 차원 축소에 효과적인 방법으로, 지도 학습 알고리즘을 적용하기 전에 전처리 단계로 사용할 수 있음.
- 클러스터 개수 k를 선택 할 때, 교차 검증에서 가장 좋은 분류 성능을 내는 값을 선택하면 됨.







### 9.1.5 군집을 사용한 준지도 학습

- 레이블이 없는 데이터가 많고 레이블이 있는 데이터는 적을 때 사용.



- 레이블이 있는 소수의 데이터만으로 학습하면 정확도가 낮게 나오고 이는 당연.
  - 이를 개선하려면, 훈련 세트를 클러스터로 모으고 각 클러스터에서 센트로이드에 가장 가까운 이미지인 **대표 이미지 Representative image** 로 학습.
    - 샘플에 레이블을 부여하는 것은 비용이 많이 들고 어려우므로, 무작위 샘플 보다는 대표 샘플에 레이블을 할당하는 것이 좋음.
      - 더 나아가, 이렇게 할당한 레이블을 동일한 클러스터에 있는 모든 샘플로 전파하는 **레이블 전파  Label propagation**을 사용해서 성능을 더 올릴 수 있음.
        - 단, 이 경우 클러스터의 경계 근처의 샘플에 잘못된 레이블이 부여되어 있을 수 있음.
          - 이를 해결하기 위해서는 센트로이드와 가까운 일부의 샘플에만 레이블을 전파하는 방법을 사용할 수 있음.



- **능동 학습 Active Learning**
  - 능동 학습에서 가장 널리 사용되는 것 중 하나인 **불확실성 샘플링 Uncertainty sampling**
    - (1) 지금까지 수집한 레이블된 샘플에서 모델을 훈련, 레이블되지 않은 모든 샘플에 대한 예측을 만듦
    - (2) 모델이 가장 불확실하게 예측한 샘플(추정 확률이 낮은 샘플) 을 전문가에게 보내 레이블을 붙임
    - (3) 레이블을 부여하는 노력만큼의 성능이 향상되지 않을 때까지 이를 반복.
  - 다른 전략은 모델을 가장 크게 바꾸는 샘플이나, 모델의 검증 점수를 가장 크게 떨어뜨리는 샘플, 여러 개의 모델이 동일한 예측을 내지 않는 샘플(SVM이나 랜덤 포레스트)에 대해 레이블을 요청하는 것.







### 9.1.6 DBSCAN

- 이 알고리즘은 밀집된 연속적 지역을 클러스터로 정의함.



- 작동방식
  - 각 샘플에서 e 거리 내에 샘플이 몇 개 있는지 셈. 이 지역을 샘플의 ***e-이웃 e-neighborhood*** 라고 함.
  - (자신을 포함한) e-이웃 내에 최소 *min-samples* 개의 샘플이 있다면 이를 ***핵심 샘플 Core instance*** 라고 함. 즉 핵심 샘플은 밀집된 지역에 있음.
  - 핵심 샘플의 이웃에 있는 모든 샘플은 동일한 클러스터. 이웃 중 핵심 샘플이 있는 경우 반복. 핵심 샘플의 이웃 중 핵심 샘플의 이웃 중 핵심 샘플의... 를 결국 하나의 클러스터로 모음.
  - 핵심 샘플이 아니고 이웃 샘플도 아닌 샘플은 이상치로 판단.



- DBSCAN은 모든 클러스터가 충분히 밀집되어있고, 밀집되어 있지 않은 지역과 잘 구분될 때 좋은 성능을 보임.
- 클러스터의 모양과 개수에 상관없이 감지 가능하고, 2개의 하이퍼파라미터(*eps*와 *min_sample*) 만 관리하면 됨.
- 하지만, 클러스터 간 밀집도가 크게 다르면 모든 클러스터를 올바르게 잡는 것이 불가능.
- 계산 복잡도는 대략 ***O(m log m)*** 으로 샘플 개수에 선형적으로 증가.
  - 사이킷런에서 구현은 eps가 커지면 ***O(m^2)*** 만큼의 메모리가 필요.







### 9.1.7 다른 군집 알고리즘

- ***병합 군집***
  - 클러스터 계층을 밑바닥부터 위로 쌓아 구성.
  - 처음에 샘플 하나에서 시작, 인접한 클러스터 쌍을 연결하는 방법.
  - 병합된 클러스터 쌍을 트리로 그리면 이진 트리가 됨.
  - 이웃한 샘플 간의 거리를 담은 m x m 크기 희소 행렬을 연결 행렬로 전달하는 식으로 대규모 샘플에도 잘 적용할 수 있음.
    - 대신 연결 행렬이 없으면 대규모 데이터셋으로 확장하기 어려움.



- ***BIRCH***
  - Balanced Iterative Reducing and Clustering using Hierarchies
  - 특별히 대규모 데이터셋을 위해 고안 됨.
  - 특성의 개수가 20개 이하로 너무 많지 않다면, 배치 k-평균보다 빠르고 비슷한 결과가 가능.
  - 제한된 메모리로 대용량 데이터셋을 다룰 수 있음.



- ***평균-이동 Mean-shift***
  - 각 샘플을 중심으로 하는 원을 그리고, 원마다 안에 포함된 모든 샘플의 평균을 구함. 그리고 원의 중심을 평균점으로 이동하며, 모든 원이 움직이지 않을 때 까지 평균-이동을 계속함.
  - 지역의 최대 밀도를 찾을 때까지 높은 쪽으로 원을 이동하는 것이며, 안착된 원 안의 모든 샘플은 동일한 클러스터가 됨.
  - DBSCAN과 유사한 특징, 모양이나 개수에 상관없이 클러스터를 찾을 수 있고, 하이퍼파라미터도 매우 적음(*밴드위스 Bandwidth* 라 부르는 원의 반경 하나 뿐)
    - DBSCAN과 다른 점은 평균-이동은 내부 밀집도가 불균형할 때 여러 개로 클러스터를 나누는 경향이 있음.
  - 계산 복잡도가 ***O(m^2)*** 로 대규모 데이터셋에 적합하지 않음.



- ***유사도 전파 Affinity propagation***
  - 투표 방식을 사용한 알고리즘
  - 샘플이 자신을 대표할 수 있는 비슷한 샘플에 투표, 각 대표와 투표한 샘플들이 클러스터를 형성.
  - 크기가 다른 여러 개의 클러스터를 감지할 수 있음.
  - 계산 복잡도가 ***O(m^2)*** 로 대규모 데이터셋에 적합하지 않음.



- ***스펙트럼 군집 Spectral clustering***
  - 샘플 사이의 유사도 행렬을 받아 저차원 임베딩을 만듦 (즉, 차원을 축소함). 그 다음 저차원 공간에서 다른 군집 알고리즘(사이킷런의 경우는 k-평균)을 수행함.
  - 복잡한 클러스터 구조를 감지하고 그래프 컷을 찾는데 사용가능. (예를 들면 sns에서 친구의 클러스터를 찾는 등.)
  - 샘플의 개수가 많거나, 클러스터의 크기가 매우 다르면 잘 작동하지 않음.







## 9.2 가우시안 혼합

- **가우시안 혼합 모델 Gaussian Mixture Model, GMM** : 샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우시안 분포에서 생성되었다고 가정하는 확률 모델
  - 하나의 가우시안 분포에서 생성된 모든 샘플이 하나의 클러스터를 형성
    - 일반적으로 클러스터는 타원형
  - 각 클러스터는 타원의 모양, 크기, 밀집도, 방향이 다름
    - 샘플이 주어지면 가우시안 분포 중 하나에서 생성되었다는 것은 알지만, 어떤 분포인지 분포의 파라미터는 무엇인지 알지 못함.



- **기댓값-최대화 Expectation-maximization, EM** 알고리즘
  - k-평균 알고리즘과 비슷
    - 클러스터 파라미터를 랜덤하게 초기화 하고 수렴할 때까지 다음 두 단계를 반복
      - (1) ***기댓값 단계 Expectation step*** : 샘플을 클러스터에 할당
      - (2) ***최대화 단계 Maximization step*** : 클러스터를 업데이트
  - k-평균과 다른 점은, 소프트 클러스터 할당이라는 점.
    - 기댓값 단계에서 각 클러스터에 속할 확률을 예측하고, 최대화 단계에서 각 클러스터가 데이터셋에 있는 모든 샘플을 사용해 업데이트.
      - 클러스터에 속할 추정 확률로 샘플에 가중치가 적용됨. 이 확률을 샘플에 대한 클러스터의 ***책임 Responsibility***라고 함.



- 가우시안 혼합 모델은 ***생성 모델 Generative model*** 으로, 모델에서 새로운 샘플을 만들 수 있음.
- 샘플이 주어지면 그 위치의 ***확률 밀도 함수 Probability Density Function, PDF***의 로그를 예측, 점수가 높을 수록 밀도가 높음.
- 특성이나 클러스터가 많거나/ 샘플이 적을 때는 EM이 최적의 솔루션으로 수렴하기 어려움.
  - 이런 어려움을 줄이려면, 알고리즘이 학습할 파라미터 개수를 제한해야 함.







### 9.2.1 가우시안 혼합을 사용한 이상치 탐지

- ***이상치 탐지 Outlier Detection*** : 보통과 많이 다른 샘플을 탐지하는 작업
  - ***이상치 Outlier*** vs ***정상치 Inlier***



- 가우시안 혼합 모델을 이상치 탐지에 사용하는 방법 : 밀도가 낮은 지역에 있는 모든 샘플을 이상치로 판단 함.
  - 이렇게 정의하려면 사용할 밀도 임곗값을 정해야 함.
    - 그리고 정밀도/재현율 트레이드 오프도 고려.
- 이와 비슷한 작업으로 ***특이치 탐지 Novelty Detection*** 이 있음.
  - 이 알고리즘은 이상치로 오염되지 않은 '*깨끗한*' 데이터셋에서 훈련한다는 것이 이상치 탐지와 다름.
- 실제로 이상치 탐지는 데이터셋 정제에 자주 사용됨.







### 9.2.2 클러스터 개수 선택하기

- k-평균 에서는 이너셔나 실루엣 점수를 사용해서 적절한 클러스터 개수를 선택했었음.
  - 이런 지표는 클러스터가 타원형이거나 크기가 다를 때 안정적이지 않으므로, 가우시안 혼합에서는 사용할 수 없음
    - 그래서 가우시안 혼합 모델에서는 다음 지표들을 사용하여
      - ***Bayesian Information Criterion, BIC***
      - ***Akaike Information Criterion, AIC***
        - 와 같은 *이론적 정보 기준 Theoretical Infomation Criterion*을 최소화 하는 모델을 선택.



- ***BIC***와 ***AIC*** 모두 학습할 파라미터가 많은(즉 클러스터가 많은 모델에게 벌칙을 가하고, 데이터에 잘 학습하는 모델에게 보상을 더함.
  - 이 둘은 종종 동일한 모델을 선택하고, 둘의 선택이 다를 경우 BIC가 선택한 모델이 AIC가 선택한 모델보다 간단한 (즉 파라미터가 적은) 경향이 있음.
    - 하지만 데이터에 아주 잘 맞지 않을 수 있음. (특히 대규모 데이터셋인 경우)







### 9.2.3 베이즈 가우시안 혼합 모델

- 최적의 클러스터 개수를 수동으로 찾지 않고 불필요한 클러스터의 가중치를 0 (또는 0에 가깝게)으로 만들 수 있음.
  - 클러스터 개수 *n_components* 를 최적의 클러스터 개수보다 크다고 생각하는 값으로 지정하고, 자동으로 불필요한 클러스터를 제거하는 알고리즘.



- **베타 분포 Beta distribution** : 고정 범위 안에 놓인 값을 가진 확률 변수를 모델링할 때 자주 사용. 0에서 1사이의 값.
- 사후 확률 분포
- 변분 추론
- 변분 파라미터
- 평균장 변분 추론
- 블랙 박스 확률적 변분 추론







### 9.2.4 이상치 탐지와 특이치 탐지를 위한 다른 알고리즘

- 사이킷런에는 이상치 탐지와 특이치 탐지 전용 알고리즘 몇 가지가 구현되어 있음

  - **PCA (그리고 *inverse_transform()* 메서드를 가진 다른 차원 축소 기법)**

    - 보통 샘플의 재구성 오차와 이상치의 재구성 오차 중 후자가 훨씬 큼.
      - 이를 이용해서 이상치 탐지 기법으로 사용함

  - **Fast-MCD, Minimum Covariance Determinant**

    - 이상치 감지에 유용, 특히 데이터셋 정제에 사용.
    - 샘플(정상치)가 혼합된 것이 아니라 하나의 가우시안 분포에서 생성되었다고 가정, 그리고 가우시안 분포에서 생성되지 않은 이상치로 이 데이터셋이 오염되었다고 가정.
    - 파라미터를 추정할 때 이상치로 의심되는 샘플은 무시
    - 타원형을 잘 추정하고 이상치를 잘 구분하도록 돕는 기법

  - **아이솔레이션 포레스트**

    - 특히 고차원 데이터셋에서 이상치 감지를 위한 효율적인 알고리즘.
    - 무작위로 성장한 결정 트리로 구성된 랜덤 포레스트를 만들고, 각 노드에서 특성을 랜덤하게 선택한 다음, 랜덤한 임곗값을 골라 데이터셋을 둘로 나눔. 이를 반복.
      - 모든 샘플이 다른 샘플과 격리될 때까지 반복.
    - 이상치는 일반적으로 다른 샘플과 멀리 떨어져 있으므로, 평균적으로 정상 샘플과 적은 단계에서 격리 됨.

  - **LOF, Local Outlier Factor**

    - 이상치 탐지에 좋음
    - 주어진 샘플 주위의 밀도와 이웃 주위의 밀도를 비교함.
    - 이상치는 종종 k개의 최근접 이웃보다 더 격리 됨.

  - **one-class SVM**

    - 특이지 탐지에 좋음
    - 모든 샘플을 고차원 공간에 매핑한 다음 선형 SVM 분류기를 사용해 두 클래스로 분리함. 여기서 샘플의 클래스가 하나이기 때문에 원본 공간으로부터 고차원 공간에 있는 샘플을 분리함. 새로운 샘플이 이 영역 안에 놓이지 않는다면 이상치로 판별.

    - 조정할 하이퍼파라미터가 적음.
    - 고차원 데이터셋에 잘 작동
    - SVM과 마찬가지로 대규모 데이터셋에 확장은 어려움.



## 9.3 연습문제

- 생략