# Ch 9_비지도 학습

- 컴퓨터 과학자 얀 르쿤이 말하길,

  > "지능이 케이크라면 비지도 학습은 케이크의 빵이고, 지도 학습은 케이크 위의 크림이고, 강화 학습은 케이크 위의 체리다."



- 대부분의 머신러닝 어플리케이션은 지도 학습 기반이지만, 실제 데이터는 대부분 레이블이 없음.
  - 레이블이 없는 데이터에 레이블을 붙이기 위해서라도 비지도 학습은 필요함.



- 비지도 학습의 예

  - **군집 Clustering** : 비슷한 샘플을 클러스터로 모음.
    - 데이터 분석, 고객 분류, 추천 시스템, 검색 엔진, 이미지 분할, 준지도 학습, 차원 축소 등에 사용.
  - **이상치 탐지 Outlier Detection** : '정상' 데이터가 어떻게 보이는지를 학습, 비정상 샘플을 감지하는 데 사용함.
    - 예를 들면, 제조 라인에서 결함 제품을 감지하거나 시계열 데이터에서 새로운 트렌드를 찾음.

  - **밀도 추정 Density Estimation** : *데이터셋 생성 확률 과정 Random process* 의 ***확률 밀도 함수 Probability Density Function, PDF*** 를 추정함.
    - 밀도 추청은 이상치 탐지에 널리 사용 됨.
      - 밀도가 매우 낮은 영역의 샘플이 이상치일 가능성이 높음.
    - 또한 데이터 분석과 시각화에도 유용.



- k-평균과 DBSCAN을 사용한 군집에서 시작, 가우시안 혼합 모델을 설명하고, 이를 밀도 추청, 군집, 이상치 탐지에 사용하는 방법을 살펴볼 것.







## 9.1 군집

- 비슷한 샘플들을 구별해, 하나의 클러스터로 할당하는 작업.
  - 분류와 비슷하지만, 분류와 달리 군집은 비지도 학습.



- *고객 분류*
  - 구매 이력이나 웹사이트 내 행동 등을 기반으로 클로스터로 모음.
    - 예를 들어 동일한 클러스터 내의 사용자가 좋아하는 컨텐츠를 추천하는, 추천 시스템



- *데이터 분석*
  - 새로운 데이터셋을 분석할 때 클러스터를 만듦



- *차원 축소 기법*
  - 데이터 셋에 군집 알고리즘을 적용, 각 클러스터에 대한 샘플의 **친화성 Affinity**를 측정할 수 있음.
    - 친화성은 샘플이 클러스터에 얼마나 잘 맞는지를 측정
  - 원본 특성 벡터보다 훨씬 저차원의 벡터로 분석에 사용



- *이상치 탐지*
  - 모든 클러스터에 친화성이 낮은 샘플은 이상치일 가능성이 높음.
    - 특히, 제조 분야에서 결함 감지, 또는 부정 거래 감지에 활용.

- *준지도 학습*
  - 레이블된 샘플이 적다면, 군집을 수행하고 레이블을 붙여, 이후의 지도 학습에 필요한 레이블을 제공할 수 있음.

- *검색 엔진*
  - 제시된 이미지와 비슷한 이미지를 찾아주는 검색 엔진을 만들 때 사용.
    - 사용자가 찾으려는 이미지를 제공하면, 훈련된 군집 모델을 사용해 이미지의 클러스터를 찾고, 클러스터의 모든 이미지를 반환.

- *이미지 분할*
  - 색을 기반으로 픽셀을 클러스터로 모은 다음, 각 픽셀의 색을 해당 클러스터의 평균 색으로 바꿈.
    - 이렇게 하면 물체의 윤관을 감지하기 쉬워져, 물체 탐지 및 추적 시스템에서 이미지 분할을 많이 활용함.



- 클러스터에 대한 보편적인 정의는 없고, 상황에 따라 다름.
  - 어떤 알고리즘은 **센트로이드 Centroid** 라 부르는 특정 포인트를 중심으로 모인 샘플을 찾고,
  - 어떤 알고리즘은 샘플이 밀집되어 연속된 영역을 찾고,
  - 어떤 알고리즘은 계층적으로 클러스터의 클러스터를 찾는 등 종류가 아주 많음.







### 9.1.1 k-평균

- k-평균은 반복 몇 번으로 데이터셋을 빠르고 효율적으로 클러스터로 묶을 수 있는 간단한 알고리즘.
- 로이드-포지 알고리즘이라고도 부름.



- 알고리즘이 찾을 클러스터 개수 k를 지정, 각 샘플은 클러스터에 할당, 군집에서 각 샘플에 붙일 레이블 label은 알고리즘이 샘플에 할당한 클러스터의 인덱스.

- k-평균 알고리즘은 클러스터의 크기가 많이 다르면 잘 작동하지 않음. 샘플을 클러스트에 할당할 때 센트오리드까지 거리를 고려하는 것이 전부이기 때문.



- **하드 군집 Hard clustering** : 샘플을 하나의 클러스터에만 할당
- **소프트 군집 Soft clustering** : 클러스터마다 샘플에 점수를 부여
  - 점수를 부여할 때, 샘플과 센트로이드 사이의 거리나 가우시안 방사 기저 함수와 같은 유사도 점수를 사용 할 수 있음.



- 고차원 데이터셋을, 샘플이 센트로이드에서 떨어져 있는 거리로 위치를 나타내는 방식으로 변환하면 k-차원 데이터셋을 만들 수 있는데, 이렇게 변환하면 매우 효율적인 비선형 차원 축소 기법이 됨.



- k-평균 알고리즘의 작동 원리는?
  - 센트로이드가 주어진다면,
    - 데이터셋의 모든 샘플에 가장 가까운 센트로이드의 클러스터를 할당하면 됨
  - 반대로 모든 샘플의 레이블이 주어진다면,
    - 각 클러스터에 속한 샘플의 평균을 계산하여 센트로이드를 구하면 됨.
  - 그럼 레이블이나 센트로이드가 주어지지 않으면?
    - 처음 센트로이드를 랜덤하게 선정하고(무작위로 k개의 샘플을 뽑아 그 위치를 센트로이드로 지정), 그 다음 샘플에 레이블을 할당하고 센트로이드를 업데이트하고, 이를 반복해서 센트로이드에 변화가 없을 때까지 반복.
- 이 알고리즘의 계산 복잡도는 일반적으로 샘플 개수 m, 클러스터 개수 k, 차원 개수 n에 선형적.
  - 하지만 데이터가 군집할 수 있는 구조일 때 그렇고, 최악의 경우 계산 복잡도는 샘플 개수가 지수적으로 증가할 수 있음.
    - 하지만 실전에서 이런 일은 드물고, 일반적으로 k-평균은 가장 빠른 군집 알고리즘 중 하나.



- 수렴은 보장되지만, 지역 최적점으로 수렴할 수 있음.
  - 이는 센트로이드 초기화에 달려있음. 센트로이드를 초기화를 개선해서 이런 위험을 줄여야 함.



- 센트로이드 초기화 방법
  - 다른 군집 알고리즘을 먼저 실행해서 센트로이드 위치의 근사값을 알거나
  - 랜덤 초기화를 다르게 하여 여러 번 알고리즘을 실행하고 가장 좋은 솔루션을 선택함
    - 이 경우 성능 지표를 사용. 각 샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리, 모델의 **이너셔 Inertia**를 사용함.

- k-평균 알고리즘을 향상시킨 k-평균++ 알고리즘에서는,
  - 다른 센트로이드와 거리가 먼 센트로이드를 선택하는 초기화 단계를 소개함.
  - 그리고 사이킷런의 KMeans 클래서는 이 초기화 방법을 사용.



- k-평균 속도 개선과 미니배치 k-평균
  - 삼각 부등식을 사용해서 불필요한 거리 계산을 많이 피했고,
  - 전체 데이터셋을 사용해 반복하지 않고, 각 반복마다 미니배치를 사용해 센트로이드를 조금씩 이동하는 방법으로 속도향상을 함.



- 최적의 클러스터 개수 찾기
  - 가장 작은 이너셔를 갖는 모델을 선택해도 결과가 좋지 않음. 이너셔는 k에 반비례하는데 k가 크다고 성능이 무조건 좋지 않음.
  - 최선의 클러스터 개수를 선택하기 위해, 하지만 계산 비용이 많이 드는, 방법은 **실루엣 점수 Silhouette score**
    - 실루엣 점수는 모든 샘플에 대한 **실루엣 계수 Silhouette coefficient**의 평균임.
      - 실루엣 계수는 (b-a) / max(a, b) 로 계산. a는 동일한 클러스터에 있는 다른 샘플까지의 평균 거리, b는 가장 가까운 클러스터까지 평균 거리를 의미.
    - 실루엣 계수는 -1에서 +1 사이의 값이며, +1에 가까우면 자신의 클러스터 안에 잘 속해 있고, 다른 클러스터와는 멀리 떨어져 있다는 의미, 실루엣 계수가 0에 가까우면 클러스터 경계에 위치, -1에 가까우면 이 샘플이 잘못된 클러스터에 할당되었다는 의미.
  - 모든 샘플의 실루엣 계수를 할당된 클러스터와 계숫값으로 정렬하여 그린 그래프를 **실루엣 다이어그램 Silhouette diagram**이라고 하며, 이 그래프의 높이는 클러스터가 포함하고 있는 샘플의 개수, 너비는 이 클러스터에 포함된 샘플의 정렬된 실루엣 계수를 의미(넓을수록 좋음).
    - 전반적인 클러스터의 크기가 비슷하고, 수직 파선보다 높은 계수를 가지는 경우를 선택해야 함.







### 9.1.2 k-평균의 한계

- k-평균은 특히 속도가 빠르고 확장이 용이한 장점이 있음.
  - 하지만, 최적이 아닌 솔루션을 피하려면 알고리즘을 여러 번 실행해야 하고, 클러스터 개수를 잘 지정해야하는 한계가 있으며, 클러스터의 크기나 밀집도가 서로 다르거나 원형이 아닌경우 잘 작동하지 않음.
    - 참고로, 타원평 클러스터에서는 가우시안 혼합 모델이 잘 작동함.
    - 이런 경우는 입력 특성의 스케일을 맞춰서 보정할 수 있음.







### 9.1.3 군집을 사용한 이미지 분할

