# Ch 7. 앙상블 학습과 랜덤 포레스트

- 대중의 지혜 Wisdom of crowd : 무작위로 사람을 많이 모아 만든 답이 전문가의 답보다 나은 경우
- **앙상블 학습 Ensenble Learning** : 예측기 여럿을 모아 만든 예측을 수집해서 더 나은 답을 찾는 학습
  - 예를 들면, 훈련 세트로 부터 무작위로 각기 다른 서브셋을 만들어 여러 결정 트리 분류기를 훈련시킨 뒤, 가장 많은 선택을 받은 클래스를 예측으로 삼는, **랜덤 포레스트 Random forest** 라는 결정 트리의 앙상블 방법이 있음.
- 사실 머신러닝 경연 대회에서는 여러 가지 앙상블 방법을 사용한 경우가 많음.
- **배깅, 부스팅, 스태킹** 등의 앙상블 방법이 있음







## 7.1 투표 기반 분류기

- 분류기의 예를 들어 보면, 로지스틱 회귀 분류기, SVM 분류기, 랜덤 포레스트 분류기, K-최근접 이웃 분류기 등이 있음.
- 더 좋은 분류기를 만들기 위해 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 방법을 사용할 수 있음. 이렇게 다수결 투표로 정해지는 분류기를 **직접 투표 hard voting** 분류기라고 함.
- 각 분류기가 랜덤 추측보다 조금 더 나은 **약한 학습기 Weak learner**일지라도 충분히 많고 다양하면 앙상블은 **강한 학습기 Strong learner** 가 될 수 있음.
  - 왜냐하면 **큰 수의 법칙 Law of large number** 때문.
- 모든 분류기가 클래스의 확률을 예측할 수 있으면 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있음. 이를 **간접 투표 Soft voting** 이라 함. 확률이 높은 투표에 비중을 더 두기 때문에 직접 투표 방식 보다 성능이 높음.







## 7.2 배깅과 페이스팅

- 각기 다른 훈련 알고리즘을 사용하거나, 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 훈련하는 방법이 있음.



- **배깅 Bagging** (bootstrap aggregating의 줄임말) : 훈련 세트에서 중복을 허용하여 샘플링하는 방식
- **페이스팅 Pasting** : 중복을 허용하지 않고 샘플링하는 방식

- 둘 모두 같은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용할 수 있음.
  - 하지만 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링할 수 있음.