# Ch 7. 앙상블 학습과 랜덤 포레스트

- 대중의 지혜 Wisdom of crowd : 무작위로 사람을 많이 모아 만든 답이 전문가의 답보다 나은 경우
- **앙상블 학습 Ensenble Learning** : 예측기 여럿을 모아 만든 예측을 수집해서 더 나은 답을 찾는 학습
  - 예를 들면, 훈련 세트로 부터 무작위로 각기 다른 서브셋을 만들어 여러 결정 트리 분류기를 훈련시킨 뒤, 가장 많은 선택을 받은 클래스를 예측으로 삼는, **랜덤 포레스트 Random forest** 라는 결정 트리의 앙상블 방법이 있음.
- 사실 머신러닝 경연 대회에서는 여러 가지 앙상블 방법을 사용한 경우가 많음.
- **배깅, 부스팅, 스태킹** 등의 앙상블 방법이 있음







## 7.1 투표 기반 분류기

- 분류기의 예를 들어 보면, 로지스틱 회귀 분류기, SVM 분류기, 랜덤 포레스트 분류기, K-최근접 이웃 분류기 등이 있음.
- 더 좋은 분류기를 만들기 위해 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 방법을 사용할 수 있음. 이렇게 다수결 투표로 정해지는 분류기를 **직접 투표 hard voting** 분류기라고 함.
- 각 분류기가 랜덤 추측보다 조금 더 나은 **약한 학습기 Weak learner**일지라도 충분히 많고 다양하면 앙상블은 **강한 학습기 Strong learner** 가 될 수 있음.
  - 왜냐하면 **큰 수의 법칙 Law of large number** 때문.
- 모든 분류기가 클래스의 확률을 예측할 수 있으면 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있음. 이를 **간접 투표 Soft voting** 이라 함. 확률이 높은 투표에 비중을 더 두기 때문에 직접 투표 방식 보다 성능이 높음.







## 7.2 배깅과 페이스팅

- 각기 다른 훈련 알고리즘을 사용하거나, 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 훈련하는 방법이 있음.



- **배깅 Bagging** (bootstrap aggregating의 줄임말) : 훈련 세트에서 중복을 허용하여 샘플링하는 방식
- **페이스팅 Pasting** : 중복을 허용하지 않고 샘플링하는 방식

- 둘 모두 같은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용할 수 있음.
  - 하지만 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링할 수 있음.



- 수집 함수는 일반적으로 분류의 경우는 **통계적 최빈값 Statistical mode** (직접 투표 분류기처럼 가장 많은 예측 결과)을, 회귀의 경우는 평균을 계산함.

- 일반적으로 개별 예측기는 훈련 세트로 훈련시킨 것보다 크게 편향되어 있지만, 수집 함수를 통과하면 편향과 분산이 모두 감소함.
  - 일반적으로 앙상블의 결과는 하나의 예측기를 훈련한 경우와 비교하면 편향은 비슷, 분산은 줄어듦.
- 각 예측기는 병렬로 학습이 가능하고, 이 때문에 배깅과 페이스팅의 인기가 높음.







### 7.2.1 사이킷런의 배깅과 페이스팅

- 사이킷런에서는 간편한 API인 ***BaggingClassifier, BaggingRegressor*** 을 제공함.
- 전반적으로 배깅이 더 나은 모델을 만들어, 선호함.







### 7.2.2 oob 평가

- 배깅을 사용하면 어떤 샘플은 한 예측기에 여러 번 샘플링되고 어떤 것은 전혀 선택되지 않을 수 있음.

- 예측기가 훈련되는 동안 oob 샘플을 사용하지 않으므로 검증 세트 말고 oob 샘플로 평가하면 됨.
  - 앙상블의 평가는 각 예측기의 oob 평가를 평균하여 얻음.







## 7.3 랜덤 패치와 랜덤 서브스페이스

- **랜덤 패치 방식 Random patches method** : 훈련 특성과 샘플을 모두 샘플링
- **랜덤 서브스페이스 방식 Random subspaces method** : 훈련 샘플을 모두 사용하고, 특성은 샘플링하는 방식.







## 7.4 랜덤 포레스트

- 랜덤 포레스트는 일반적으로 배깅(또는 페이스팅) 을 적용한 결정 트리의 앙상블.

- 트리의 노드를 분할할 때, 전체 특성 중에서 최선의 특성을 찾는 대신, 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로, 무작위성을 더 주입.
  - 이는 트리를 더욱 다양하게 만들고, 편향을 손해보는 대신 분산을 낮추어, 전체적으로 더 나은 모델을 만듦.







### 7.4.1 엑스트라 트리

- 랜덤 포레스트에서 트리를 더 무작위하게 만들기 위해 최적의 임곗값을 찾는 대신 후보 특성을 사용해 무작위로 분할한 다음 그 중에서 최상의 분학을 선택.
  - 이와 같이 극단적으로 무작위한 트리의 랜덤 포레스트를 **익스트림 랜덤 트리 Extremely randomized trees** 앙상블, 줄여서 **엑스트라 트리 Extra-trees**라 부름
    - 여기서도 마찬가지로 편향이 늘어나지만 분산은 줄어듦.
    - 가장 최적의 임곗값을 찾지 않으므로, 일반적인 랜덤 포레스트보다 엑스트라 트리가 훨씬 빠름.







### 7.4.2 특성 중요도

- 랜덤 포레스트의 또 다른 장점은 특서으이 상대적 중요도를 측정하기 쉽다는 것.
  - 어떤 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정함.
    - 정확히는 가중치 평균. 가중치는 연관된 훈련 샘플의 수.







## 7.5 부스팅

- **부스팅 Boosting (원래는 *가설 부스팅 hypothesis boosting*  이라 불렸음.)** : 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법.
  - 앞의 모델을 보완해가면서 일련의 예측기를 학습시키는 것.



- 부스팅 방법에는 여러 가지가 있지만,
  - 가장 인기 있는 것은 **에이다부스트 AdaBoost (Adaptive Boosting의 줄임말)** 와 **그레이디언트 부스팅 Gradient Boosting**







### 7.5.1 에이다부스트

- 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높여 이전 예측기를 보완하는 방법.
  - 이렇게 하면 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 됨.
- 샘플의 가중치를 업데이트 하면서 순차적으로 학습.
- 경사 하강법과 비슷하지만, 경사 하강법은 비용 함수를 최소화하기 위해 한 예측기의 모델 파라미터를 조정해가는 반면, 에이다부스트는 점차 더 좋아지도록 앙상블에 예측기를 추가함.
- 단점으로는 병렬화를 할 수 없어, 결국 배깅이나 페이스팅만큼 확장성이 높지 않은 점이 있음.
- 사이킷런은 SAMME 라는 에이다부스트의 다중 클래스 버전을 사용. (클래스가 2개 뿐일 때는 에이다부스트와 같음.)







### 7.5.2 그레이디언트 부스팅

- 에이다부스트처럼 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가함. 하지만 반복마다 샘플의 가중치를 수정하는 에이다부스트와 달리 이전 예측기가 만든 **잔여 오차 Residual Error**에 새로운 예측기를 학습시키는 차이가 있음.



- **그레이디언트 트리 부스팅** 또는 **그레이디언트 부스티드 회귀 트리 Gradient Boosted Regression Tree, GRBT** : 결정 트리를 기반 예측기로 사용하는 회귀

- **축소 Shrinkage** : 각 트리의 기여 정도를 낮게 설정, 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만, 일반적으로 예측의 성능은 좋아짐.
- **확률적 그레이디언트 부스팅 Stochastic gradient boosting** : 각 트리의 훈련 샘플을 무작위로 선택해서 학습, 편향이 높아지는 대신 분산이 낮아지게 됨. 훈련 속도가 상당히 빨라지는 장점이 있음.







## 7.6 스태킹

- **스태킹 Stacking (Stacked generalization의 줄임말)**
- "앙상블에 속한 모든 예측기의 예측을 취합하는 함수를 사용하는 대신, 취합하는 모델을 훈련시킬 수는 없을까?"에서 시작
  - 마지막 취합하는 예측기를 **블렌더 Blender** 또는 **메타 학습기 Meta learner**라고 함.







## 7.7 연습문제

- 생략
- 