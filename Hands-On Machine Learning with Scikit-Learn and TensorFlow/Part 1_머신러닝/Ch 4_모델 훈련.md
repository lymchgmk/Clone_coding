# Ch 4. 모델 훈련

- 그 동안 내부 작동 원리는 모르고 많은 일을 처리
- 어떻게 작동하는지 이해하고 있으면 적절한 모델, 올바른 훈련 알고리즘, 좋은 하이퍼파라미터 빠르게 찾기 등이 가능.
- 디버깅이나 에러 분석에도 도움.
- 신경망 이해, 구축, 훈련에 필수



- 가장 간단한 모델 중 하나인 선현 회구부터 시작. 모델 훈련에 두가지 방법이 있음.
  - (1) 직접 계산할 수 있는 공식을 사용해서 훈련 세트에 가장 잘 맞는 모델 파라미터를 해석적으로 구함.
  - (2) **경사 하강법(GD)** 이라 불리는 반복적인 최적화 방식을 사용.
    - 모델 파라미터를 조금씩 바꾸면서 비용함수를 훈련 세트에 대해 최소화.
    - 경사하강법의 변종으로, **배치 경사하강법, 미니배치 경사하강법, 확률적 stochastic 경사 하강법(SGD)** 이 있음
- 비선형 데이터셋에 훈련시킬 수 있는 모델은 **다항 회귀**
  - **선형 회귀** 보다 파라미터가 많아서 과대적합되기 쉬운 문제가 있음.
    - 따라서, **학습 곡선 learning curve**를 사용해 모델이 과대적합되었는지 감지한 후, 과대적합을 감소시킬 수 있는 규제 기법을 사용해야 함.
- 분류 작업에 널리 사용하는 로지스틱 회귀와 소프트맥스 회귀도 살펴 볼 것.







## 4.1 선형 회귀

- 일반적으로 선형 모델은 입력 특성의 가중치 합 + **편향 Bias** 해서 예측을 만듦.
- 벡터 형태로 간단하게 표기
  - 종종 벡터를 **열 벡터 column vector**로 나타내는데, 이 때 **전치 transpose** 시켜 사용.
    - 예측 결과는 같지만 스칼라 값이 아닌 하나의 원소를 가진 행렬이 만들어짐.



- 모델을 훈련시킨다는 것은 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정한다는 것.
  - 모델이 훈련 데이터에 얼마나 잘 들어맞는지 측정 (회귀에서는 RMSE를 가장 널리 사용)
    - 실제로는 RMSE 보다 MSE를 최소화하는 것이 같은 결과를 내면서 더 간단함.







### 4.1.1 정규방정식

- **정규방정식 Normal equation** : 비용 함수를 최소화하는 theta 값을 찾기 위한 **해석적인 방법**



- **유사역행렬 Pseudoinverse** (정확히는, 무어-펜로즈 역행렬)
  - 유사역행렬 자체는 **특잇값 분해, Singular Value Decomposition (SVD)** 라 부르는 표준 행렬 분해기법을 사용해 계산.
    - 정규방정식을 계산하는 것보다 이 방식이 훨씬 효율적







### 4.1.2 계산 복잡도

- 정규방정식의 **계산 복잡도, Computational complexivity** 는 일반적으로 O(n^2.4)에서 O(n^3) 사이.
  - 특성 수가 2배로 늘어나면 계산 시간이 5.3에서 8배 증가.
    - 정규방정식과 SVD 모두 특성 수가 많아지면 매우 느려지지만, 훈련 세트의 샘플 수에 대해서는 선형적으로 O(m) 증가하므로 메모리 공간이 허락된다면 큰 훈련 세트도 효율적으로 처리할 수 있음.



- 다음 방법은 특성이 매우 많고 훈련 샘플이 너무 많에 메모리에 모두 담을 수 없을 때 적합.







## 4.2 경사 하강법

- 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘
- 경사 하강법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것.
- 그래이디언트가 0이 되면 최솟값에 도달한 것.



- **무작위 초기화 random initialization**으로 시작, theta를 임의의 값으로 시작해서, 그래이디언트를 계산, 비용 함수가 감소하는 방향으로 진행하여, 알고지름이 최솟값에 수렴할 때 까지 점진적으로 향상.
  - 경사 하강법에서 중요한 파라미터는 스텝(반복적인 학습 알고리즘에서 학습의 각 단계)의 크기
    - **학습률 learning rate** 하이퍼 파라미터로 결정
      - 학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 함.
      - 학습률이 너무 크면 골짜기를 가로질러 반대편으로 건너뛰어, 이전보다 더 높은 곳으로 올라가게 될지도 모름. 이러면 값이 발산하여 적절한 해법을 찾지 못하게 함.



- 모든 비용 함수가 매끈하지 않고, 최솟값으로 수렴하기 매우 어려운 경우가 있음. 이로 인해 두 가지 문제점이 생김.
  - (1) **전역 최솟값 global minimum** 보다 덜 좋은 **지역 최솟값 local minimum** 에 수렴
  - (2) 기울기가 0 이지만 최솟값이 아닌 구간, 평지에서 학습이 멈춰버림
- 선형 회귀에 사용하는 MSE 비용 함수는 볼록 함수, 지역 최솟값이 없고 하나의 전역 최솟값만 존재하며 연속된 함수이고 기울기가 갑자기 변하지 않으므로, 경사 하강법의 전역 최솟값 수렴이 보장됨.
  - **립시츠 연속 Lipschitz continuous** : 어떤 함수의 도함수가 일정한 범위 안에서 변할 때.
    - MSE는 x가 무한대일 떄 기울기가 무한대가 되므로 국부적인 립시츠 함수라고 함.



- 경사 하강법을 사용할 때는 **반드시 모든 특성이 같은 스케일을 갖도록** 만들어야 함.
  - 그러지 않으면 수렴하는데 훨씬 시간이 오래 걸림. (그래이디언트의 변화가 매우 줄어들어버리기 때문)



- 모델 훈련은 훈련 세트에서 비용 함수를 최소화하는 모델 파라미터의 조합을 찾는 일이며, 이를 모델의 **파라미터 공간 parameter space**에서 찾는다고 말함.
  - 파라미터가 많을 수록 공간의 차원이 커지고 검색이 더 어려워짐.







### 4.2.1 배치 경사 하강법

- 경사 하강법을 구현하려면 각 모델 파라미터에 대해 비용 함수의 그레이디언트를 계산해야 함.
  - 즉, theta가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산해야 함.
  - **편도함수 partial derivative** 를 구해야함.

- 편도함수를 각각 계산하지말고 그레이디언트 벡터를 사용해서 한꺼번에 계산 함.
  - 비용함수의 그레이디언트 벡터를 구하는 공식은 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 계산하며, 때문에 이 알고리즘을 **배치 경사 하강법 Batch gradient descent**라고 함.
    - 때문에 매우 큰 훈련 세트에서는 아주 느리지만, 특성 수에 민감하지 않은 장점이 있음.
    - 수십만 개 수준의 특성의 경우에 선형 회귀를 훈련시키려면, 정규방정식이나 SVD 분해 보다는 경사 하강법이 훨씬 빠름.
- 그레이디언트 벡터를 구한 뒤 내려가는 스텝의 크기를 곱해서 값을 결정함.



- 적절한 학습률을 찾기 위해서는 그리드 탐색을 사용하지만, 그리드 탐색에서 수렴하는 데 너무 오래 걸리는 모델을 막기 위해 반복 횟수를 제한해야 함.
  - 반복 횟수는 너무 작으면 최적점에 도달 전에 알고리즘이 멈추고 / 너무 크면 모델 파라미터가 더는 변하지 않는 동안 시간을 낭비하게 됨.
    - 해결법은 반복 횟수를 아주 크게 지정하고, 그레이디언트 벡터가 아주 작아지면 (= 벡터의 norm이 허용오차 tolerance) 보다 작아지면 경사 하강법이 최솟값에 충분히 근접했으므로 알고리즘을 중지하면 됨.
      - 보통 허용오차를 1/10로 줄이면 알고리즘의 반복은 10배 늘어남







### 4.2.2 확률적 경사 하강법

- 배치 경사 하강법의 가장 큰 문제는 매 스텝에서 전체 훈련 세트를 사용하므로, 훈련 세트가 커지면 매우 느려진다는 점.
  - 반면 **확률적 경사 하강법**은 매 스텝에서 한 개의 샘플을 무작위로 선택하고, 그 하나의 샘플에 대한 그레이디언트를 계산함.
    - 매 반복에서 다뤄야 할 데이터가 매우 적기 때문에 알고리즘이 훨씬 빠르고, 메모리 점유가 적기 때문에 매우 큰 훈련 세트도 훈련시킬 수 있음. (SGD는 외부 메모리 학습 알고리즘으로 구현할 수 있음.)
- 빠른 대신, 확률적이므로 배치 경사 하강법보다 훨씬 불안정한 단점이 있음.
  - 비용 함수가 최솟값에 다다를 때까지 위아래로 요동치며, 평균적으로 수렴함.