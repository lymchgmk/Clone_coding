# Ch 6. 결정 트리

- **결정 트리**는 SVM 처럼 분류, 회귀, 다중출력도 가능한 머신러닝 알고리즘이며, 매우 복잡한 데이터셋도 학습할 수 있는 강력한 알고리즘임.

- 최근 자주 사용되는 가장 강력한 머신러닝 알고리즘 중 하나인 **랜덤 포레스트** 의 기본 구성 요소이기도 함.







## 6.1 결정 트리 학습과 시각화

- 사이킷 런의 **export_graphviz()** 함수를 사용해 그래프 정의를 .dot 파일로 출력하여 훈련된 결정 트리를 시각화 할 수 있음.







## 6.2 예측하기

- **루트 노드 root node** : 깊이가 0인 맨 꼭대기의 노드
- **자식 노드 child node**
- **리프 노드 leaf node** : 자식 노드를 가지지 않는 노드



- 결정 트리의 여러 장점 중 하나, 데이터 전처리가 거의 필요하지 않음.
  - 특성의 스케일을 맞추거나, 평균을 원점에 맞추는 작업이 필요없음.



- **불순도 Impurity** : 한 노드의 모든 샘플이 같은 클래스에 속해 있다면 이 노드를 순수(gini = 0)라고 함.



- 사이킷런은 이진 트리만 만드는 CRAT 알고리즘을 사용하기 때문에, 리프 노드 외의 모든 노드는 자식 노드를 2개 씩 가짐. 다른 알고리즘, ID3 같은 알고리즘을 쓴다면 둘 이상의 자식 노드를 가진 결정 트리를 만들 수 있음.



- **화이트박스 White box**모델 vs **블랙박스 Black box**모델
  - **화이트박스** 모델 : 결정 트리처럼 직관적이고 결정 방식을 이해하기 쉬움
  - **블랙박스** 모델 : 랜덤 포레스트나 신경망. 왜 그런 예측을 만드는지는 쉽게 설명하기 어려운 모델.







## 6.3 클래스 확률 추정

- 결정 트리는 한 샘플이 특정 클래스 k에 속할 확률을 추정할 수 있음.
- 샘플의 리프 노드를 찾기 위해 트리를 탐색하고, 그 노드에 있는 클래스 k의 훈련 샘플의 비율을 반환함.







## 6.4 CART 훈련 알고리즘

- 사이킷런은 결정 트리를 훈련시키기 위해 **CART, Classification and Regression Tree** 알고리즘을 사용함.
  - 하나의 특성 k의 임곗값 tk를 사용해 두 개의 서브셋으로 나누고, 가장 순수한 서브셋으로 나눌 수 있는 (k, tk) 짝을 찾음. 그리고 이를 반복해서 서브셋을 계속 둘로 나눔. 최대 깊이가 되거나 불순도를 줄이는 분할을 찾을 수 없으면 멈춤.
  - CART 알고리즘은 **탐욕적 알고리즘 Greedy algorithm** 임. 맨 위 루트노드에서 최적의 분할을 그리디하게 찾으며 각 단계에서 반복. 이는 납득할 만한 솔루션이지만 최적의 솔루션은 아님.
    - 불행하게도 최적의 트리를 찾는 것은 **NP-완전 NP-complete** 문제이며 시간복잡도가 ***O(exp(m))*** 으로 매우 작은 훈련 세트에도 적용하기 어려움. 그러니 납득할 만한 솔루션에 만족해야 함.







## 6.5 계산 복잡도

- 결정 트리를 탐색하기 위해서는 ***O(log2(m))*** 개의 노드를 거쳐야 함.
  - 각 노드는 하나의 특성값만 확인하기 때문에 특성의 수에 관계 없이 예측에 필요한 전체 복잡도는 같음.







## 6.6 지니 불순도 또는 엔트로피?

- 기본적으로 지니 gini 불순도가 사용되지만, 엔트로피 불순도를 사용할 수 있음.
- 엔트로피 불순도에서는 안정되고 질서 정연하면 엔트로피가 0에 가까움.
- 섀넌의 **정보이론**에서는 메시지의 평균 정보 양을 측정하는데, 모든 메시지가 동일할 때 엔트로피가 0이 됨.
- 머신러닝에서는 어떤 세트가 한 클래스의 샘플만 담고 있다면 엔트로피가 0.



- 지니 불순도와 엔트로피는 실제로는 큰 차이가 없음.
  - 지니 불순도가 조금 더 계산이 빠르기 때문에 기본값으로 좋음
  - 대신 지니 불순도는 가장 빈도 높은 클래스를 한쪽 가지로 고립시키는 경향이 있고, 엔트로피는 조금 더 균형 잡힌 트리를 만듦.







## 6.7 규제 매개변수

- 결정 트리는 훈련 데이터에 대한 제약 사항이 거의 없음.
- 그리고 결정 트리는 모델 파라미터의 수가 훈련되기 전에 결정되지 않는, **비파라미터 모델 Nonparametric model** 이라 불림.
  - 모델 구조가 데이터에 맞춰지기 때문에, 고정되지 않고 자유로움
- 반면 **파라미터 모델 Parametric model** 은 미리 정의된 모델 파라미터 수를 가지므로, 자유도가 제한되고 과대적합될 위험이 줄어듦 (대신 과소적합될 위험은 커짐)



- 과대적합을 피하기 위해 결정 트리의 자유도를 제한할 필요가 있는데, 이를 알다시피 규제라고 함.
  - 결정 트리에서는 보통 최대 깊이는 많이 제어함.
  - 그 외에는 분할되기 위해 노드가 가져야 하는 최소 샘플 수,
  - 리프 노드가 가지고 있어야 할 최소 샘플 수,
  - 가중치가 부여된 전체 샘플 수에서의 비율,
  - 리프 노드의 최대 수,
  - 각 노드에서 분할에 사용할 특성의 최대 수 등이 있음.
  - **min_** 으로 시작하는 매개변수를 증가 / **max_** 로 시작하는 매개변수를 감소시키면 모델의 규제가 커짐.







## 6.8 회귀

- 결정 트리는 회귀 문제에도 사용할 수 있음.

- CART 알고리즘에서 불순도 대신 MSE를 최소화하도록 분할하는 것을 제외하고는 똑같이 작동.
- 과대적합을 피하기 위해서는 규제.







## 6.9 불안정성

- 결정 트리는 이해하고 해석하기 쉬우며, 사용하기 편하고, 여러 용도로 사용할 수 있고, 성능도 뛰어남
- 하지만, 결정 트리는 계단 모양의 결정 경계를 만들고, 이는 훈련 세트의 회전에 민감한 단점을 가짐.
  - 이를 해결하는 한 방법은 훈련 데이터를 더 좋은 방향으로 회전시키는 PCA 기법을 사용하는 것임.
- 다음 장의 랜덤 포래스트는 많은 트리의 예측을 평균하여 이런 불안정성을 극복함.







## 6.10 연습문제

생략