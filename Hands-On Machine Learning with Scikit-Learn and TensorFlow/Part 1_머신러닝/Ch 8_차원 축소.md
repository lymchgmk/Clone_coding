# Ch 8_차원 축소

- **차원의 저주 Curse of dimensionality** : 샘플 각각의 특성이 수백만에게 달해서, 훈련을 느리게 할 뿐만 아니라, 좋은 솔루션을 찾기 힘들게 함.
  - 이를 해결하기 위해서 특성의 수를 크게 줄여버림. 이를 차원 축소라 함.



- 차원 축소에 사용되는 두 가지 주요 접근 방법 : **투영 Projection** 과 **매니폴드 학습 Manifold learning**
- 가장 인기 있는 차원 축소 기법 : PCA, 커널 PCA, LLE







## 8.1 차원의 저주

- 고차원 데이터셋은 매우 희박할 위험이 있음. (대부분의 훈련 데이터가 서로 멀리 떨어져 있음.)
  - 이 경우 예측을 위해 훨씬 많은 외삽을 해야하고, 저차원일 때보다 예측이 불안정, 즉 훈련 세트의 차원이 클 수 록 과대적합 위험이 커짐.
    - 이론적으로 차원의 저주를 해결하는 방법 하나는, 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우는 것. 그러나 실제로는 필요한 훈련 샘플의 수가 기하급수적으로 늘어나기 때문에 사실상 불가능한 방법.
      - 그래서 훈련 세트의 크기를 키우지 않고 차원을 축소시킬 수 밖에 없음.







## 8.2 차원 축소를 위한 접근 방법

- 투영과 매니폴드 학습







### 8.2.1 투영

- 대부분의 실전 문제는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져있지 않고, 특성들이 서로 강하게 연결되어 있음.
  - 결과적으로 모든 훈련 샘플이 고차원 공간 안의 저차원 **부분 공간 Subspace**에 놓여 있음.
- 예를 들면 3차원 공간에서 데이터들이 거의 특정 평면을 이루면서 모여 있음.
  - 이를 수직으로 투영시켜서 데이터를 분석하면, 데이터셋의 차원을 3D에서 2D로 낮춘 것.
  - 하지만 차원 축소에 있어서 투영이 언제나 최선의 방법은 아님.
    - **스위스 롤 Swiss roll** 데이터셋처럼 부분 공간이 뒤틀리거나 휘어있기도 하기 때문.
      - 그래서 이경우는 매니폴드를 사용함.







### 8.2.2 매니폴드 학습

- 위의 스위스 롤은 2D 매니폴드의 한 예.



- 일반적으로, d차원 매니폴드는 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부 (d < n)
  - 훈련 샘플을 이런 매니폴드로 모델링하는 식으로 차원 축소하는 알고리즘을 **매니폴드 학습 Manifold learning** 이라 함.
    - 이는 대부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴으에 가깝게 노형 있다는 **매니폴드 가정** 또는 **매니폴드 가설**에 근거함.







## 8.3 PCA

- **주성분 분석 Principal component analysis, PCA** : 데이터에 가장 가까운 초평면을 정의한 다음, 데이터를 이 평면에 투영시키는 방법







### 8.3.1 분산 보존

- 데이터를 투영한 여러 축 중, 분산이 최대로 보존되는, 정보가 가장 적게 손실된 축을 선택하는게 합리적일 것.
  - 이를 다르게 말하면, 원본 데이터셋과 투영된 것 사이의 평균 제곱 거리를 최소화하는 축을 선택하는 것을 의미.







### 8.3.2 주성분

- PCA는 훈련 세트에서 분산이 최대인 축을 찾고, 그 축에 수직이면서 남은 분산을 최대한 보존하는 두 번째 축을 찾고, ... , n 번째 축을 찾음.
  - i 번째 축을 이 데이터의 i번째 **주성분 Principal component, PC**라고 부름.
    - 주성분을 찾는 방법은 특잇값 분해 Singular Value Decompositon, SVD 라는 표준 행렬 분해 기술로 주성분의 단위 벡처를 찾는 방법을 사용함.







### 8.3.3 d차원으로 투영하기

- 주성분을 모두 추출한 뒤, 처음 d개의 주성분으로 정의한 초평면에 투영하여 데이터셋을 d차원으로 축소할 수 있음.







### 8.3.4 사이킷런 사용하기

- 사이킷런의 PCA 모델은 SVD 분해 방법을 사용하여 구현.







### 8.3.5 설명된 분산의 비율

- **설명된 분산의 비율 Explained variance ratio** : 각 주성분의 축을 따라 있는 데이터셋의 분산 비율.







### 8.3.6 적절한 차원 수 선택하기

- 충분한 분산이 될 때까지 더해야 할 차원 수를 선택하는 것이 간단함.
  - 단, 데이터 시각화를 위해 축소하는 경우는 이해하기 쉽게 2, 3차원 정도가 적당함.







### 8.3.7 압축을 위한 PCA

- 차원을 축소한 뒤에는 훈련 세트의 크기가 줄어들고, 그로 인해 분류 알고리즘의 속도를 크게 높일 수 있음.

- 반대로, 압축된 데이터셋에 PCA 투영의 변환을 반대로 적용해서 차원의 갯수를 되돌릴 수도 있음.
  - 대신 투영에서 일정량의 분산을 잃어버렸기 때문에 원본 데이터셋을 얻을 수는 없고, 충분히 비슷한 데이터는 얻을 수 있음.
    - 이때 원본 데이터와 재구성(압축 후 원복)한 데이터 사이의 평균 제곱 거리를 **재구성 오차 Reconstruction Error**라 함.







### 8.3.8 랜덤 PCA

