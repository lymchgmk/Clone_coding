# Ch 8_차원 축소

- **차원의 저주 Curse of dimensionality** : 샘플 각각의 특성이 수백만에게 달해서, 훈련을 느리게 할 뿐만 아니라, 좋은 솔루션을 찾기 힘들게 함.
  - 이를 해결하기 위해서 특성의 수를 크게 줄여버림. 이를 차원 축소라 함.



- 차원 축소에 사용되는 두 가지 주요 접근 방법 : **투영 Projection** 과 **매니폴드 학습 Manifold learning**
- 가장 인기 있는 차원 축소 기법 : PCA, 커널 PCA, LLE







## 8.1 차원의 저주

- 고차원 데이터셋은 매우 희박할 위험이 있음. (대부분의 훈련 데이터가 서로 멀리 떨어져 있음.)
  - 이 경우 예측을 위해 훨씬 많은 외삽을 해야하고, 저차원일 때보다 예측이 불안정, 즉 훈련 세트의 차원이 클 수 록 과대적합 위험이 커짐.
    - 이론적으로 차원의 저주를 해결하는 방법 하나는, 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우는 것. 그러나 실제로는 필요한 훈련 샘플의 수가 기하급수적으로 늘어나기 때문에 사실상 불가능한 방법.
      - 그래서 훈련 세트의 크기를 키우지 않고 차원을 축소시킬 수 밖에 없음.







## 8.2 차원 축소를 위한 접근 방법

- 투영과 매니폴드 학습







### 8.2.1 투영

- 대부분의 실전 문제는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져있지 않고, 특성들이 서로 강하게 연결되어 있음.
  - 결과적으로 모든 훈련 샘플이 고차원 공간 안의 저차원 **부분 공간 Subspace**에 놓여 있음.
- 예를 들면 3차원 공간에서 데이터들이 거의 특정 평면을 이루면서 모여 있음.
  - 이를 수직으로 투영시켜서 데이터를 분석하면, 데이터셋의 차원을 3D에서 2D로 낮춘 것.
  - 하지만 차원 축소에 있어서 투영이 언제나 최선의 방법은 아님.
    - **스위스 롤 Swiss roll** 데이터셋처럼 부분 공간이 뒤틀리거나 휘어있기도 하기 때문.
      - 그래서 이경우는 매니폴드를 사용함.







### 8.2.2 매니폴드 학습

- 위의 스위스 롤은 2D 매니폴드의 한 예.



- 일반적으로, d차원 매니폴드는 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부 (d < n)