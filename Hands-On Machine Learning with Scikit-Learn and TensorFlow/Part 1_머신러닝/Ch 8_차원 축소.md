# Ch 8_차원 축소

- **차원의 저주 Curse of dimensionality** : 샘플 각각의 특성이 수백만에게 달해서, 훈련을 느리게 할 뿐만 아니라, 좋은 솔루션을 찾기 힘들게 함.
  - 이를 해결하기 위해서 특성의 수를 크게 줄여버림. 이를 차원 축소라 함.



- 차원 축소에 사용되는 두 가지 주요 접근 방법 : **투영 Projection** 과 **매니폴드 학습 Manifold learning**
- 가장 인기 있는 차원 축소 기법 : PCA, 커널 PCA, LLE







## 8.1 차원의 저주

- 고차원 데이터셋은 매우 희박할 위험이 있음. (대부분의 훈련 데이터가 서로 멀리 떨어져 있음.)
  - 이 경우 예측을 위해 훨씬 많은 외삽을 해야하고, 저차원일 때보다 예측이 불안정, 즉 훈련 세트의 차원이 클 수 록 과대적합 위험이 커짐.
    - 이론적으로 차원의 저주를 해결하는 방법 하나는, 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우는 것. 그러나 실제로는 필요한 훈련 샘플의 수가 기하급수적으로 늘어나기 때문에 사실상 불가능한 방법.
      - 그래서 훈련 세트의 크기를 키우지 않고 차원을 축소시킬 수 밖에 없음.







## 8.2 차원 축소를 위한 접근 방법

- 투영과 매니폴드 학습







### 8.2.1 투영

- 대부분의 실전 문제는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져있지 않고, 특성들이 서로 강하게 연결되어 있음.
  - 결과적으로 모든 훈련 샘플이 고차원 공간 안의 저차원 **부분 공간 Subspace**에 놓여 있음.
- 예를 들면 3차원 공간에서 데이터들이 거의 특정 평면을 이루면서 모여 있음.
  - 이를 수직으로 투영시켜서 데이터를 분석하면, 데이터셋의 차원을 3D에서 2D로 낮춘 것.
  - 하지만 차원 축소에 있어서 투영이 언제나 최선의 방법은 아님.
    - **스위스 롤 Swiss roll** 데이터셋처럼 부분 공간이 뒤틀리거나 휘어있기도 하기 때문.
      - 그래서 이경우는 매니폴드를 사용함.







### 8.2.2 매니폴드 학습

- 위의 스위스 롤은 2D 매니폴드의 한 예.



- 일반적으로, d차원 매니폴드는 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부 (d < n)
  - 훈련 샘플을 이런 매니폴드로 모델링하는 식으로 차원 축소하는 알고리즘을 **매니폴드 학습 Manifold learning** 이라 함.
    - 이는 대부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴으에 가깝게 노형 있다는 **매니폴드 가정** 또는 **매니폴드 가설**에 근거함.







## 8.3 PCA

- **주성분 분석 Principal component analysis, PCA** : 데이터에 가장 가까운 초평면을 정의한 다음, 데이터를 이 평면에 투영시키는 방법







### 8.3.1 분산 보존

- 데이터를 투영한 여러 축 중, 분산이 최대로 보존되는, 정보가 가장 적게 손실된 축을 선택하는게 합리적일 것.
  - 이를 다르게 말하면, 원본 데이터셋과 투영된 것 사이의 평균 제곱 거리를 최소화하는 축을 선택하는 것을 의미.







### 8.3.2 주성분

- PCA는 훈련 세트에서 분산이 최대인 축을 찾고, 그 축에 수직이면서 남은 분산을 최대한 보존하는 두 번째 축을 찾고, ... , n 번째 축을 찾음.
  - i 번째 축을 이 데이터의 i번째 **주성분 Principal component, PC**라고 부름.
    - 주성분을 찾는 방법은 특잇값 분해 Singular Value Decompositon, SVD 라는 표준 행렬 분해 기술로 주성분의 단위 벡처를 찾는 방법을 사용함.







### 8.3.3 d차원으로 투영하기

- 주성분을 모두 추출한 뒤, 처음 d개의 주성분으로 정의한 초평면에 투영하여 데이터셋을 d차원으로 축소할 수 있음.







### 8.3.4 사이킷런 사용하기

- 사이킷런의 PCA 모델은 SVD 분해 방법을 사용하여 구현.







### 8.3.5 설명된 분산의 비율

- **설명된 분산의 비율 Explained variance ratio** : 각 주성분의 축을 따라 있는 데이터셋의 분산 비율.







### 8.3.6 적절한 차원 수 선택하기

- 충분한 분산이 될 때까지 더해야 할 차원 수를 선택하는 것이 간단함.
  - 단, 데이터 시각화를 위해 축소하는 경우는 이해하기 쉽게 2, 3차원 정도가 적당함.







### 8.3.7 압축을 위한 PCA

- 차원을 축소한 뒤에는 훈련 세트의 크기가 줄어들고, 그로 인해 분류 알고리즘의 속도를 크게 높일 수 있음.

- 반대로, 압축된 데이터셋에 PCA 투영의 변환을 반대로 적용해서 차원의 갯수를 되돌릴 수도 있음.
  - 대신 투영에서 일정량의 분산을 잃어버렸기 때문에 원본 데이터셋을 얻을 수는 없고, 충분히 비슷한 데이터는 얻을 수 있음.
    - 이때 원본 데이터와 재구성(압축 후 원복)한 데이터 사이의 평균 제곱 거리를 **재구성 오차 Reconstruction Error**라 함.







### 8.3.8 랜덤 PCA

- 확률적 알고리즘을 사용해 처음 d개의 주성분에 대한 근삿값을 찾음.







### 8.3.9 점진적 PCA

- PCA 구현의 문제는 SVD 알고리즘을 실행하기 위해 전체 훈련 세트를 메모리에 올려야 한다는 것.
  - 이를 해결하기 위해 **점진적 PCA Incremental PCA** 알고리즘이 개발됨.
    - 훈련 세트를 미니배치로 나눈 뒤 IPCA 알고리즘에 한 번에 하나씩 주입, 이런 방식은 훈련 세트가 클 때 유용하고 온라인으로 PCA를 사용할 수도 있음.







## 8.4 커널 PCA

- 앞서 SVM 에서 비선형 분류와 회귀를 가능하게 하는 수학적 기법이었던 커널 트릭을 PCA에 적용해서 차원 축소를 위한 복잡한 비선형 투형을 수행. 이를 **커널 PCA, kPCA**라고 함.
  - 이 기법은 투영된 후에 샘플의 군집을 유지하거나, 꼬인 매니폴드에 가까운 데이터셋을 펼치게 만들 때도 유용.







### 8.4.1 커널 선택과 하이퍼파라미터 튜닝

- kPCA는 비지도 학습이기 때문에 성능 측정 기준이 없음.
  - 하지만 차원 축소는 지도 학습의 전처리 단계로 활용되므로 그리드 탐색을 사용하여 주어진 문제에서 성능이 가장 좋은 커널과 하이퍼파라미터를 선택할 수 있음.

- **재구성 원상 Pre-image** 을 얻어서 원본 샘플과의 제곱 거리를 측정, 재구성 원상의 오차를 최소화하는 커널과 하이퍼파라미터를 선택.







## 8.5 LLE

- **지역 선형 임베딩 Locally Linear Embedding, LLE** : 또 다른 **비선형 차원 축소 Nonlinear dimensionality Reduction, NLDR** 기술로,  이전 알고리즘과 달리 투영에 의존하지 않는 매니폴드 학습.
  - LLE는 각 훈련 샘플이 가장 가까운 이웃에 얼마나 선형적으로 연관되어 있는지 측정 후, 국부적인 관계가 가장 잘 보존되는 훈련 세트의 저차원 표현을 찾음.
    - 이 방법은 잡음이 너무 많지 않는 경우에 꼬인 매니폴드를 펼치는 데 특히 잘 작동함.



- 사이킷런이 제공하는 LLE 구현의 계산 복잡도는
  - k개의 가장 가까운 이웃을 찾는데 : *O(mlog(m)nlog(k))*
  - 가중치 최적화에 : *O(mnk^3)*
  - 저차원 표현을 만드는 데 : *O(dm^2)*
    - 마지막 항의 m^2 때문에, 대량의 데이터셋에 적용하기는 어려운 알고리즘







## 8.6 다른 차원 축소 기법

- **랜덤 투영 Random Projection** : 랜덤한 선형 투영을 사용해 데이터를 저차원 공간으로 투영.
  - 이상하게 들리지만 실제로 수학적 증명을 통해, 이런 랜덤 투영이 거리를 잘 보존하는 것으로 밝혀짐.



- **다차원 스케일링 Multidimensional Scaling, MDS** : 샘플 간의 거리를 보존하면서 차원을 축소



- **Isomap** : 각 샘플을 가장 가까운 이웃과 연결하는 식으로 그래프를 만들고, 샘플 간의 **지오데식 거리 Geodesic distance**를 유지하며 차원을 축소.



- **t-SNE, t-distributed Stochastic Neighbor Embedding** : 비슷한 샘플은 가까이, 비슷하지 않은 샘플은 멀리 떨어지도록 하면서 차원을 축소. 주로 시각화에 많이 사용되며, 특히 고차원 공간에 있는 샘플의 군집을 시각화할 때 사용.



- **선형 판별 분석 Linear Discriminant Analysis, LDA** : 사실 분류 알고리즘이지만, 훈련 과정에서 클래스 사이를 가장 잘 구분하는 축을 학습하며, 이 축은 데이터가 투영되는 초평면을 정의하는 데 사용할 수 있음.
  - 장점은 투영을 통해 가능한 클래스를 멀리 떨어지게 유지시키므로, SVM 분류기 같은 다른 분류 알고리즘을 적용하기 전 차원 축소에 좋음.







## 8.7 연습문제

- 생략