# Ch 5. 서포트 벡터 머신

- **서포트 벡터 머신 Support vector machine, SVM** : 선형, 비선형 분류 / 회귀 / 이상치 탐색 에도 사용할 수 있는 다목적 머신러닝 모델
  - 머신러닝에서 가장 인기 있는 모델 중 하나이며, 필수
  - 특히 복잡한 분류 문제에 잘 맞고, 작거나 중간 크기의 데이터셋에 적합







## 5.1 선형 SVM 분류

- 두 개의 클래스를 나누고 있을 뿐만 아니라 가장 가까운 훈련 샘플로 부터 가능한 멀리 떨어져 있는 결정 경계를 가짐. 클래스 사이의 가장 폭이 넓은 도로와 같음. 그래서 **라지 마진 분류 Large Margin Classification** 이라고도 함.

- **서포트 벡터 Support vector** : 도로 경계를 결정하는 벡터. 도로 경계에 가장 가깝고, 두 데이터 셋을 대표한다고 할 수 있음.







### 5.1.1 소프트 마진 분류

- 모든 샘플이 도로 바깥쪽에 올바르게 분류되어 있는 경우를 **하드 마진 분류 Hard Margin Classification**이라 함.
  - 하드 마진 분류에는 2 가지 문제점이 있음
    - (1) 데이터가 선형적으로 구분될 수 있어야 제대로 작동하며,
    - (2) 이상치에 민감함

- 이런 문제를 피하기 위해 좀 더 유연한 모델 (이상치가 있어도 괜찮은)이 필요함.

  - 충분히 넓은 도로의 폭을 유지하는 것.

  - **마진 오류 Margin violation** : 샘플이 도로 중간이나 심지어 반대쪽에 있는 경우.
  - 위의 2가지 조건 사이의 적절한 균형을 잡아야함.
    - 이를 **소프트 마진 분류 Soft Margin Classification** 이라 함.







## 5.2 비선형 SVM 분류

- 선형적으로 분류할 수 없는 테이터셋이 실제로는 대부분임.
  - 비선형 데이터셋을 다루는 한 방법은 다항 특성과 같은 특성을 더 추가하는 것.
    - 특성을 추가해서 선형적으로 구분되는 데이터셋으로 만들어버리면 됨.







### 5.2.1 다항식 커널

- 다항식 특성을 추가하는 것은 간단하고 모든 머신러닝 알고리즘에서 잘 작동함.
- 하지만 낮은 차수의 다항식은 매우 복잡한 데이터셋을 표현 못하고 / 높은 차수의 다항식은 모델을 느리게 만듦.
- SVM에서는 이를 해결하기 위해 **커널 트릭 Kernel trick**이라는 수학적 기교를 사용함. 매우 좋음.
  - 실제로는 특성을 추가하지 않으면서 다항식 특성을 많이 추가한 것과 같은 결과를 얻을 수 있는 기교.







## 5.2.2 유사도 특성

- 비선형 특성을 다루는 또 다른 기법
  - 각 샘플이 특정 **랜드마크 Landmark** 와 얼마나 닮았는지 측정하는 **유사도 함수 Similarity function**으로 계산한 특성을 추가하는 방법.
    - 유사도 함수의 예를 들면 **방사 기저 함수 Radial Basis Function, RBF**
      - 차원을 늘려서 변환된 훈련 세트에서 선형적으로 구분될 가능성을 높이는 기법
      - 단점은 특성을 늘려 변환하기 때문에 느려짐.







### 5.2.3 가우시안 RBF 커널

- 추가 특성을 모두 계산 하려면 연산 비용이 많이 드는 문제가 있음.
  - 이를 해결하기 위해 가우시안 RBF 커널을 사용한 SVC모델을 사용함.
    - gamma 값과 C 값을 변화시켜서 모델의 복잡도를 조정함.
    - 다른 커널도 있지만 거의 사용되지 않음. 어떤 커널은 특정 데이터 구조에 특화되어 있긴 함.
      - **문자열 커널 String Kernel** 은 가끔 텍스트 문서나 DNA 서열 분류에 사용. (**문자열 서브시퀀스 커널 String subsequence kernel**, **레벤슈타인 거리 Levenshtein distance** 기반의 커널)



- 여러 가지 커널 중 어떤 것을 사용?
  - 경험적으로 언제나 선형 커널을 가장 먼저 시도하는게 좋음.
    - 특히 훈련 세트가 크거나 특성이 많을 때
  - 훈련 세트가 너무 크지 않다면 가우시안 RBF도 시도하면 좋음.
  - 시간과 컴퓨팅 성능이 충분하다면 교차 검증, 그리드 탐색
  - 특정 훈련 데이터의 구조에 특화된 커널이 있다면 해당 커널을 선택.







### 5.2.4 계산 복잡도

- liblinear 라이브러리는 대략 ***O(m\*n)***
- 정밀도를 높이면 알고리즘의 수행 시간이 길어지는데, 허용오차 하이퍼파라미터 e로 조절.
- libsvm 라이브러리는 ***O(m\*n)*** 에서 ***O(m\*n)*** 사이
  - 훈련 샘플 수가 커지면 엄청나게 느려짐. 복잡하지만 작거나 중간 규모의 훈련 세트에 잘 맞음.
    - 하지만, 특성의 개수는 특희 **희소 특성 Sparse features**, 각 샘플에 0이 아닌 특성이 몇 개 없는 경우, 잘 확장 됨.
      - 알고리즘의 성능이 샘플이 가진 0이 아닌 특성의 평균 수에 거의 비례.







## 5.3 SVM 회귀

- 앞서 말한 것처럼, SVM 알고리즘은 다목적 사용 가능.
  - 선형, 비선형 분류 / 선형, 비선형 회귀
- 분류가 아닌 회귀에 적용하는 방법은 목표를 반대로 하는 것.
  - 일정한 마진 오류 안에서 두 클래스 간의 도로 폭이 최대가 되게 하는 대신 / 제한된 마진 오류 안에서 도로 안에 가능한 많은 샘플이 들어가도록 학습.
    - 도로의 폭은 하이퍼파라미터 e로 조절
      - 마진 안에서는 훈련 샘플이 추가되어도 모델의 예측에 영향이 없음.
        - 그래서 이 모델을 **e에 민감하지 않다 e-insensitive** 라고 말함.







## 5.4 SVM 이론

