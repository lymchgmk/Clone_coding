# Ch 10. 케라스를 사용한 인공 신경망 소개

- 지능적인 기계를 만들기 위해 뇌 구조를 모방
  - **인공 신경망 Artificial Neural Network (ANN)**
    - 딥러닝의 핵심, 다재다능하고 강력하고 확장성이 좋음.
      - 이미지 분류
      - 음성 인식 서비스의 성능 높이기
      - 비디오 추천
      - 딥마인드의 알파고
    - 아주 복잡한 대규모 머신러닝 문제를 다루는데 적합함.



- **다중 퍼셉트론 Multi-layer perceptron (MLP)**







## 10.1 생물학적 뉴런에서 인공 뉴런까지

- 인공 신경망은 1943년 신경생리학자 위런 매컬러 Warren McCulloch와 수학자 월터 피츠 Walter Pitts의 논문에서 처음 소개됨
- 침체기와 부흥, 다시 침체기와 부흥을 맞이함.
  - 신경망 훈련에 사용할 데이터가 많아짐
    - 인공 신경망은 규모가 크고 복잡한 문제에서 다른 머신러닝 기법보다 좋은 성능을 냄
  - 컴퓨터 하드웨어가 크게 발전, 무어의 법칙 Moore's Law 덕분
    - 게임 산업 덕분에 수백만 개의 강력한 GPU가 생산,
    - 클라우드 플랫폼의 환경이 제공됨
  - 훈련 알고리즘이 향상됨
    - 작은 변경이지만 커다란 영향을 끼침
  - 이론상의 제한이 실전에서 문제가 되지 않음이 밝혀짐
    - 지역 최적점에 갇혀서 해결책을 찾을 수 없을 것이라 생각했지만, 실제로는 이런일이 매우 드묾 (지역 최적점에 도달하더라도 일반적으로 전역 최적점에 매우 가까움)
      - 고차원 공간에서는 대부분 안장점같으며, 지역 최적점은 매우 드물고, 만약 있다면 전역 최적점에 가까운 값임이 밝혀짐
  - 투자와 진보의 선순환에 들어감







### 10.1.1 생물학적 뉴런

- **생물학적 신경망 Biological Neural Network (BNN)**







### 10.1.2 뉴런을 사용한 논리 연산

- 생물학적 뉴런에서 착안한 매우 단순한 신경망 모델, 인공뉴런
  - 하나 이상의 이진(on/off) 입력과 이진 출력 하나를 가짐
  - 단순히 입력이 일정 개수만큼 활성화되었을 때, 출력을 내보냄
  - 이런 간단한 모델로 네트워크를 만들면, 어떤 논리 명제도 계산할 수 있다는 것이 증명됨.
    - 논리곱, 논리합, 논리부정 등







### 10.1.3 퍼셉트론

- **퍼셉트론 Perceptron** : 가장 간단한 인공 신경망 구조 중 하나, 1957년 프랑크 로젠블라트 Frank Rosenblatt 가 제안.
  - **TLU (Threshold Logic Unit)** 또는 **LTU (Linear Threshold Unit)** 라고 불리는 인공 뉴런을 기반으로 함.
    - 입력과 출력이 이진이 아닌 어떤 숫자이고, 각각의 입력 연결은 가중치과 연관되어 있음
    - 입력의 가중치 합을 계산한 뒤, 계산된 합에 **계단 함수 Step function**을 적용하여 결과를 출력.
  - 일반적으로, **헤비사이드 계단 함수 Heaviside step function**을 가장 널리 사용함
    - 가끔 **부호 함수 sign function**을 대신 사용하기도 함
  - 간단한 선형 이진 분류 문제에 사용할 수 있음.
    - 입력의 선형 조합을 계산, 임곗값을 넘으면 양성 클래스를 출력, 임곗값을 넘지 않으면 음성 클래스를 출력.
      - 예를 들면, 꽃잎의 길이와 너비를 기반으로 붓꽃의 품종을 분류
    - 이 경우 **TLU**를 훈련한다는 것은 최적의 w0, w1, w2, ... 를 찾는다는 뜻
  - **퍼셉트론**은 층이 하나뿐인 **TPU**로 구성됨
    - 각 **TLU**는 모든 입력에 연결 됨
    - **완전 연결 층 Fully connected layer)** 또는 밀집층 **Dense layer** : 한 층에 있는 모든 뉴련이 이전 층의 모든 뉴런과 연결되어 있을 때
  - 보통 입력층은 입력 뉴런으로 구성되고, 편향 뉴런이 추가됨



- **헤브의 규칙 Hebb's rule** : 퍼셉트론이 훈련되는 방식. 뉴런이 동시에 활성화될 때마다 이들 사이의 연결 가중치가 증가. 오차가 감소되도록 연결을 강화시킴



- **퍼셉트론 수렴 이론 Perceptron convergence theorem** : 로젠블라트가 증명. 훈련 샘플이 선형적으로 구분될 수 있다면 이 알고리즘이 정답에 수렴함.



- 퍼셉트론을 여러 개 쌓아올리면 일부 제약을 줄일 수 있음, 이런 인공 신경망을 **다층 퍼셉트로 MLP** 라고 함.
  - **다층 퍼셉트론**은 XOR 문제를 풀 수 있음







### 10.1.4 다층 퍼셉트론과 역전파

- 다층 퍼센트론은 입력층 하나와 **은닉층 hidden layer** 이라 불리는 하나 이상의 TLU층과 마지막 출력층으로 구성 됨.
- 입력층과 가까운 층을 하위 층, 출력에 가까운 층을 상위 층
- 출력층을 제외한 모든 층은 편향 뉴련을 포함하고, 다음 층과 완전히 연결됨.



- 은닉층을 여러 개 쌓아 올린 인공 신경망을 **심층 신경망 Deep Neural Network (DNN)**

  - 1990년대에는 은닉층이 2개 이상인 경우를 DNN이라 봄, 그러나 현대에는 은닉층이 수십에서 수백 개인 신경망이 흔해서 기준이 애매함.

  - **딥러닝**은 심층 신경망을 연구하는 분야이며, 일반적으로는 연산이 연속하여 길게 연결된 모델 연구를 의미.
    - 그러나 많은 사람들은 얕더라도 신경망이 사용되면 딥러닝이라고 함



- 다층 퍼셉트론을 훈련하기 위해 오랫동안 성공못하다, 1986년 데이비드 루멜하트, 제프리 힌턴, 로날드 윌리엄스가 **역전파 Backpropagation** 훈련 알고리즘을 소개함.
  - **역전파** 알고리즘은 효율적인 기법으로 gradient를 자동으로 계산하는 경사 하강법임.
  - 네트워크를 두 번 (정방향 + 역방향) 통과하는 것만으로 모든 모델 파라미터에 대한 네트워크 오차의 gradient를 계산할 수 있음.
    - = 오차를 감소시키기 위해 각 연결 가중치와 편향값이 어떻게 바뀌어야 할지 알 수 있음.
    - gradient를 구한 뒤, 평범한 경사 하강법을 수행
    - 네트워크가 어떤 해결책으로 수렴될 때까지 반복
      - 자동으로 gradient를 계산하는 것을 **자동 미분 automatic differnentiation, audodiff** 라고 부름
        - 여러 자동 미분 기법 중, 역전파에서는 **후진 모드 자동 미분 reverse-mode autodiff** 를 사용.
          - 이 기법은 빠르고 정확, 미분할 함수가 변수가 많고 출력이 적은 경우 잘 맞음.



- 정리하면,
  - (1) 각 훈련 샘플에 대해 역전파 알고리즘이 먼저 예측을 만들고 (정방향 계산)
  - (2) 역방향으로 각 층을 거치면서 각 연결이 오차에 기여한 정도를 측정하고 (역방향 계산)
  - (3) 마지막으로 이 오차가 감소하도록 가중치를 조정함. (경사 하강법 단계)



- 이 알고리즘을 계선하기 위해, 계단 함수를 로지스틱(시그모이드) 함수로 변경함.
  - 계단 함수는 수평선 밖에 없으니 계산할 그레이디언트가 없지만,
  - 로지스틱 함수는 어디서든지 0이 아닌 그레이디언트가 잘 정의되어 있기 때문.
    - 확설화 함수는 보통 다음의 두가지를 사용함.
      - (1) ***하이퍼볼릭 탄젠트 함수 (쌍곡 탄젠트 함수)***
        - 로지스틱 함수처럼 S자 모양이고 연속적이며 미분 가능
        - 하지만 -1에서 1사이의 값 (로지스틱은 0에서 1사이)
        - 훈련 초기에 각 층의 출력을 원점 근처로 모으는 경향이 있고, 이는 종종 빠르게 수렴되도록 도와줌.
      - (2) ***ReLU 함수***
        - 연속적이지만, z=0에서 미분 가능하지 않음 (기울기가 갑자기 변해서 경사 하강법이 이상한 방향으로 튈 수 있음)
        - z < 0 일 때 도함수가 0이지만, 실제로 잘 작동하고 계산 속도가 빠른 장점이 있어서, 기본 활성화 함수로 사용 중.
        - 가장 중요한 점은, 출력에 최댓값이 없다는 점이 경사 하강법에 있는 일부 문제를 완화해줌. (Ch 11 참고)



- 활성화 함수가 필요한 이유?
  - 선형 변환은 여러 개 연결해도 선형 변환 뿐이기 때문.
    - 따라서, 층 사이에 비선형성을 추가하지 않으면, 아무리 층을 많이 쌓아도 하나의 층이나 마찬가지 결과가 나옴.
      - 이러면 복잡한 문제를 풀 수 없음
    - 반대로 비선형 활성화 함수가 있는 충분히 큰 심층 신경망은 이론적으로 어떤 연속 함수도 근사 가능.







### 10.1.5 회귀를 위한 다층 퍼셉트론

- 다층 퍼셉트론은 회귀 작업에 사용할 수 있음.
  - 값 하나를 예측하는데 출력 뉴런이 하나만 필요, 이 출력 뉴런이 예측된 값이면 됨.
  - 반면 *다변량 회귀 Multivariate regression* 에서는 동시에 (여러 값을 예측하는 경우) 출력 차원마다 출력 뉴련이 하나씩 필요함.
    - 예를 들면 이미지에서 물체의 중심 위치를 파악하는 경우, 2D좌표를 예측하려면 x와 y 좌표 2개의 값이 출력되어야 함.



- 일반적으로 회귀용 다층 퍼셉트론을 만들 때는 출력 뉴런에 활서오하 함수를 사용하지 않고 어떤 범위의 값도 출력되도록 함.
  - 하지만 출력이 항상 양수여야 한다면 출력층에 ReLU 활성화 함수를 사용하거나 softmax 활성화 함수를 사용할 수 있음.
    - *softplus 활성화 함수* : ReLU 함수의 변종으로, 
      - z < 0 일때 0에 가까워지고
      - z >> 0 일 수록 z에 가까움
  - 어떤 범위 안의 값을 예측하고 싶다면 로지스틱 함수나 하이퍼볼릭 탄젠트 함수를 사용하고 레이블의 스케일을 적절한 범위로 조절.
    - 로지스틱 함수는 0에서 1 사이를 출력하고,
    - 하이퍼볼릭 탄젠트 함수는 -1에서 1 사이를 출력함.



- 손실 함수는 일반적인 경우는 MSE를, 훈련 세트에 이상치가 많다면 MAE를 사용함.
  - 또는 이 둘을 조합한 *후버 Huber* 손실을 사용할 수 있음
    - 후버 손실은 오차가 임곗값 (전형적으로 1) 보다 작을 때는 2차 함수, 임곗값보다 클 때는 선형 함수.
      - 선형 함수 부분은 MSE 보다 이상치에 덜 민감하고,
      - 2차 함수 부분은 MAE 보다 빠르고 정확하게 수렴함.







### 10.1.6 분류를 위한 다층 퍼셉트론

- 다층 퍼셉트론은 분류에도 사용.
- ***이진 분류 Binary Classification*** 문제에서는 로지스틱 활성화 함수를 가진 하나의 출력 뉴련만 있으면 됨
  - 0과 1 사이의 실수를 출력.
    - 이를 양성 클래스에 대한 예측 확률로,
    - 1- (양성 클래스 예측 확률) 을 음성 클래스에 대한 예측 확률로 사용하면 됨.



- 다층 퍼셉트론으로 ***다중 레이블 이진 분류 Multilabel Binary Classification*** 문제를 쉽게 처리 가능함.
  - 예를 들어, 이메일이 스팸인지 아닌지 예측하고, 동시에 긴급한 메일인지 아닌지 예측하는 경우.
    - 출력 뉴런 2개가 각각 스팸 확률과 긴급한 메일 확률을 출력하면 됨.
      - 출력된 확률의 합이 1이 될 필요는 없음.
      - 모델은 어떤 레이블 조합도 출력할 수 있음.
        - 긴급하지 않은 메일, 긴급한 메일, 긴급하지 않은 스팸 메일, 심지어 긴급한 스팸 메일도 가능.



- 3개 이상의 클래스 중 한 클래스에만 속할 수 있다면, 클래스마다 하나의 출력 뉴련이 필요한 경우임.
  - 이 경우, 출력층에 소프트맥스 활성화 함수를 사용해야 함.
    - 소프트맥스 함수는 모든 예측확률을 0과 1사이로 만들고, 모두 더했을 때 1이 되도록 함. (클래스가 배타적인 경우 필요함)
      - 이를 ***다중 분류 Multiclass Classification*** 라고 함.
  - 확률 분포를 예측해야 하므로, 손실 함수는 일반적으로 *크로스 엔트로피 손실 Cross-entropy loss (로그 손실 Log Loss 라고도 함)* 을 사용.



- 즉 분류 MLP는 이진 분류, 다중 레이블 분류, 다중 분류에 사용가능 함.







## 10.2 케라스로 다층 퍼셉트론 구현하기

- 케라스는 모든 종류의 신경망을 손쉽게 만들고, 훈련 / 평가 / 실행 할 수 있는 고수준 딥러닝 API임.
- 텐서플로에 번들로 포함된 tf.keras를 사용할 것.



- 텐서플로우 1.x 버전은 복잡했지만, 텐서플로우 2는 파이토치만큼 간결하게 정리 됨.

- 과거 파이토치는 이식성이 제한적이고 계산 그래프의 해석이 없는 단점이 있었지만 파이토치 1.0 버전에서 많이 보완함.







### 10.2.1 텐서플로 2 설치

- virtualenv, pip로 설치

- 생략







### 10.2.2 시퀀셜 API를 사용하여 이미지 분류기 만들기

- 이미지 *MNIST* 데이터를 사용할 것.