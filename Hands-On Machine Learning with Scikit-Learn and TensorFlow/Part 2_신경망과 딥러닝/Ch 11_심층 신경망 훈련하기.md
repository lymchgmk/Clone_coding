# Ch 11_심층 신경망 훈련하기

- 앞선 심층 신경망들은 몇 개의 은닉층 만드로 이루어진 얕은 네트워크 였음.
  - (고해상도 이미지에서 수백 종류에 물체를 감지하는 것 같은) 아주 복잡한 문제라면, 수백 개의 뉴런과 10개 이상의 층과 수십만 개의 가중치가 필요할 것.



- 매우 깊은 심층 신경망 훈련에서는 다음과 같은 문제가 발생할 수 있음.
  - **그레이디언트 소실** 또는 **그레이디언트 폭주**
    - 심층 신경망의 아래쪽으로 갈수록 그레이디언트가 점점 더 작아지거나 커지는 현상.
      - 하위층의 훈련을 힘들게 만드는 문제.
  - 훈련 데이터가 충분하지 않거나 레이블을 만드는 작업이 힘든 경우.
  - 훈련이 매우 느릴 수 있음.
  - 수백만 개의 파라미터를 가지기 때문에, 훈련세트에 과대적합될 위험이 매우 큼.
    - 특히 훈련 샘플이 충분하지 않거나, 잡음이 많은 경우.







## 11.1 그레이디언트 소실과 폭주 문제

- 역전파 알고리즘은 출력층에서 입력층으로 오차 그레이디언트를 전파하면서 진행 됨.
  - 알고리즘이 신경망의 모든 파라미터에 대한 오차 함수의그레이디언트를 계산하면, 경사 하강법 단계에서 이 그레이디언트를 사용하여 각 파라미터를 수정.
- 그런데, 알고리즘이 하위층으로 진행될수록 그레이디언트가 점점 작아지는 경우가 많음.
  - 경사 하강법이 하위층의 연결 가중치를 변경되지 않은 채로 둔다면 훈련이 좋은 솔루션으로 수렴하지 않을 것.
    - 이를 **그레이디언트 소실 Vanishing Gradient** 라고 함.
- 반대로, 그레이디언트가 점점 커져서 여러 층이 비정상적으로 큰 가중치로 갱신되어, 알고리즘이 발산 Diverse 하면,
  - 이를 **그레이디언트 폭주 Exploding Gradient** 라고 함.
    - 그레이디언트 폭주는 보통 순환 신경망에서 나타남.



- 오랫동안 심층 신경망 훈련에서, 그레이디언트를 불안정하게 만드는 원인이 명확하게 밝혀지지 않았음.
  - 로지스틱 시그모이드 활성화 함수와 가중치 초기화 방법 (평균이 0이고 표준편차가 1인 정규분포) 에서 출력의 분산이 입력의 분산보다 더 큰 문제를 발견하게 됨.







### 11.1.1 글러럿과 He 초기화

