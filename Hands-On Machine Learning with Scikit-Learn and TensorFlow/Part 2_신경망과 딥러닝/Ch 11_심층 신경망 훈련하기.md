# Ch 11_심층 신경망 훈련하기

- 앞선 심층 신경망들은 몇 개의 은닉층 만드로 이루어진 얕은 네트워크 였음.
  - (고해상도 이미지에서 수백 종류에 물체를 감지하는 것 같은) 아주 복잡한 문제라면, 수백 개의 뉴런과 10개 이상의 층과 수십만 개의 가중치가 필요할 것.



- 매우 깊은 심층 신경망 훈련에서는 다음과 같은 문제가 발생할 수 있음.
  - **그레이디언트 소실** 또는 **그레이디언트 폭주**
    - 심층 신경망의 아래쪽으로 갈수록 그레이디언트가 점점 더 작아지거나 커지는 현상.
      - 하위층의 훈련을 힘들게 만드는 문제.
  - 훈련 데이터가 충분하지 않거나 레이블을 만드는 작업이 힘든 경우.
  - 훈련이 매우 느릴 수 있음.
  - 수백만 개의 파라미터를 가지기 때문에, 훈련세트에 과대적합될 위험이 매우 큼.
    - 특히 훈련 샘플이 충분하지 않거나, 잡음이 많은 경우.







## 11.1 그레이디언트 소실과 폭주 문제

- 역전파 알고리즘은 출력층에서 입력층으로 오차 그레이디언트를 전파하면서 진행 됨.
  - 알고리즘이 신경망의 모든 파라미터에 대한 오차 함수의그레이디언트를 계산하면, 경사 하강법 단계에서 이 그레이디언트를 사용하여 각 파라미터를 수정.
- 그런데, 알고리즘이 하위층으로 진행될수록 그레이디언트가 점점 작아지는 경우가 많음.
  - 경사 하강법이 하위층의 연결 가중치를 변경되지 않은 채로 둔다면 훈련이 좋은 솔루션으로 수렴하지 않을 것.
    - 이를 **그레이디언트 소실 Vanishing Gradient** 라고 함.
- 반대로, 그레이디언트가 점점 커져서 여러 층이 비정상적으로 큰 가중치로 갱신되어, 알고리즘이 발산 Diverse 하면,
  - 이를 **그레이디언트 폭주 Exploding Gradient** 라고 함.
    - 그레이디언트 폭주는 보통 순환 신경망에서 나타남.



- 오랫동안 심층 신경망 훈련에서, 그레이디언트를 불안정하게 만드는 원인이 명확하게 밝혀지지 않았음.
  - 로지스틱 시그모이드 활성화 함수와 가중치 초기화 방법 (평균이 0이고 표준편차가 1인 정규분포) 에서 출력의 분산이 입력의 분산보다 더 큰 문제를 발견하게 됨.







### 11.1.1 글러럿과 He 초기화

- 글로럿과 벤지오가 논문에서 불안정한 그레이디언트 문제를 크게 완화하는 방법을 제안함.

  - 예측을 할 때는 정방향으로, 그레이디언트를 역전파할 때는 역방향으로,
  - 양방향 신호가 적절하게, 신호가 죽거나 폭주 또는 소멸하지 않아야 함.
    - 적절한 신호가 흐르려면, 각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 함.
    - 그리고 역방향에서 층을 통과하기 전과 후의 그레이디언트 분산이 동일해야함.

  - 사실 층의 입력 연결 개수 (***팬-인 Fan-in***) 와 출력 연결 개수 (***팬-아웃 Fan-out***) 가 같지 않다면 위의 두 가지를 보장할 수 없음.
    - 이를 해결하기 위해 글로럿과 벤지오는 각 층의 연결 가중치를 무작위로 초기화하는 방법을 제안함.
      - 이 초기화 전략을 **세이비어초기화 Xavier initialization** 또는 **글로럿 초기화 Gloroy initialization** 라고 함.
        - 그리고 fan avg 를 fan in 으로 바꾸면 얀 르쿤이 제안한, **르쿤 초기화 LuCun initialization** 라고 함.
      - 글로럿 초기화를 사용하면 훈련 속도를 상당히 높일 수 있고, 이는 현재 딥러닝의 성공을 견인한 기술 중 하나.



- 다른 활성화 함수들에 대해서도 비슷한 전략이 제안되었고,
  - 그 중 ReLU 활성화 함수 (와 그 변종들) 에 대한 초기화 전략을 **He 초기화 He initialization** 라고 함.



- 정리하면
  - 글로럿
    - 활성화 함수 : 활성화 함수 없음, 하이퍼볼릭 탄텐트,로지스틱, 소프트맥스
    - 정규분포 : 1 / fan avg
  - He
    - 활성화 함수 : ReLU 함수와 그 변종들
    - 정규분포 : 2 / fan in
  - 르쿤
    - 활성화 함수 : SELU
    - 정규분포 : 1 / fan in







### 11.1.2 수렴하지 않는 활성화 함수

